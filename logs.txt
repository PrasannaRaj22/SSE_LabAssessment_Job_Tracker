
==> Audit <==
┌────────────┬────────────────────────────────────────┬──────────┬──────────────────────┬─────────┬─────────────────────┬─────────────────────┐
│  COMMAND   │                  ARGS                  │ PROFILE  │         USER         │ VERSION │     START TIME      │      END TIME       │
├────────────┼────────────────────────────────────────┼──────────┼──────────────────────┼─────────┼─────────────────────┼─────────────────────┤
│ start      │                                        │ minikube │ PRASANNA-NITRO\ADMIN │ v1.37.0 │ 14 Oct 25 15:45 IST │ 14 Oct 25 16:18 IST │
│ docker-env │                                        │ minikube │ PRASANNA-NITRO\ADMIN │ v1.37.0 │ 14 Oct 25 16:18 IST │ 14 Oct 25 16:18 IST │
│ docker-env │ minikube docker-env --shell powershell │ minikube │ PRASANNA-NITRO\ADMIN │ v1.37.0 │ 14 Oct 25 16:19 IST │ 14 Oct 25 16:19 IST │
│ docker-env │ minikube docker-env --shell powershell │ minikube │ PRASANNA-NITRO\ADMIN │ v1.37.0 │ 14 Oct 25 16:19 IST │ 14 Oct 25 16:19 IST │
│ service    │ todo-service                           │ minikube │ PRASANNA-NITRO\ADMIN │ v1.37.0 │ 14 Oct 25 16:21 IST │                     │
│ docker-env │                                        │ minikube │ PRASANNA-NITRO\ADMIN │ v1.37.0 │ 15 Oct 25 11:02 IST │                     │
│ docker-env │ minikube docker-env --shell powershell │ minikube │ PRASANNA-NITRO\ADMIN │ v1.37.0 │ 15 Oct 25 11:03 IST │                     │
│ start      │                                        │ minikube │ PRASANNA-NITRO\ADMIN │ v1.37.0 │ 15 Oct 25 11:03 IST │ 15 Oct 25 11:04 IST │
│ docker-env │                                        │ minikube │ PRASANNA-NITRO\ADMIN │ v1.37.0 │ 15 Oct 25 11:04 IST │ 15 Oct 25 11:04 IST │
│ service    │ jobtracker-service                     │ minikube │ PRASANNA-NITRO\ADMIN │ v1.37.0 │ 15 Oct 25 11:07 IST │                     │
└────────────┴────────────────────────────────────────┴──────────┴──────────────────────┴─────────┴─────────────────────┴─────────────────────┘


==> Last Start <==
Log file created at: 2025/10/15 11:03:51
Running on machine: Prasanna-Nitro
Binary: Built with gc go1.24.6 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1015 11:03:51.726926   40096 out.go:360] Setting OutFile to fd 116 ...
I1015 11:03:51.728390   40096 out.go:374] Setting ErrFile to fd 120...
W1015 11:03:51.751274   40096 root.go:314] Error reading config file at C:\Users\ADMIN\.minikube\config\config.json: open C:\Users\ADMIN\.minikube\config\config.json: The system cannot find the file specified.
I1015 11:03:51.771646   40096 out.go:368] Setting JSON to false
I1015 11:03:51.774303   40096 start.go:130] hostinfo: {"hostname":"Prasanna-Nitro","uptime":154676,"bootTime":1760351755,"procs":333,"os":"windows","platform":"Microsoft Windows 11 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.26200.6725 Build 26200.6725","kernelVersion":"10.0.26200.6725 Build 26200.6725","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"316e415b-390b-4729-b0a0-b0858e96c28b"}
W1015 11:03:51.774303   40096 start.go:138] gopshost.Virtualization returned error: not implemented yet
I1015 11:03:51.774817   40096 out.go:179] 😄  minikube v1.37.0 on Microsoft Windows 11 Home Single Language 10.0.26200.6725 Build 26200.6725
I1015 11:03:51.775910   40096 notify.go:220] Checking for updates...
I1015 11:03:51.776461   40096 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1015 11:03:51.778054   40096 driver.go:421] Setting default libvirt URI to qemu:///system
I1015 11:03:51.860936   40096 docker.go:123] docker version: linux-28.5.1:Docker Desktop 4.48.0 (207573)
I1015 11:03:51.866279   40096 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1015 11:03:52.106289   40096 info.go:266] docker info: {ID:b24ccc47-792f-41a6-a448-1ca3602492c1 Containers:3 ContainersRunning:2 ContainersPaused:0 ContainersStopped:1 Images:5 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:46 OomKillDisable:false NGoroutines:104 SystemTime:2025-10-15 05:33:52.085793713 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:7959953408 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.39] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.44] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Users\ADMIN\.docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-mcp.exe] ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.23.0] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.42] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I1015 11:03:52.106820   40096 out.go:179] ✨  Using the docker driver based on existing profile
I1015 11:03:52.107348   40096 start.go:304] selected driver: docker
I1015 11:03:52.107348   40096 start.go:918] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1015 11:03:52.107348   40096 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1015 11:03:52.116915   40096 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1015 11:03:52.343505   40096 info.go:266] docker info: {ID:b24ccc47-792f-41a6-a448-1ca3602492c1 Containers:3 ContainersRunning:2 ContainersPaused:0 ContainersStopped:1 Images:5 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:46 OomKillDisable:false NGoroutines:104 SystemTime:2025-10-15 05:33:52.326665291 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:7959953408 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.39] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.44] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Users\ADMIN\.docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-mcp.exe] ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.23.0] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.42] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I1015 11:03:52.397292   40096 cni.go:84] Creating CNI manager for ""
I1015 11:03:52.398356   40096 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1015 11:03:52.398921   40096 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1015 11:03:52.399467   40096 out.go:179] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I1015 11:03:52.400003   40096 cache.go:123] Beginning downloading kic base image for docker with docker
I1015 11:03:52.400003   40096 out.go:179] 🚜  Pulling base image v0.0.48 ...
I1015 11:03:52.401070   40096 image.go:81] Checking for docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I1015 11:03:52.401070   40096 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1015 11:03:52.401070   40096 preload.go:146] Found local preload: C:\Users\ADMIN\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1015 11:03:52.401599   40096 cache.go:58] Caching tarball of preloaded images
I1015 11:03:52.401599   40096 preload.go:172] Found C:\Users\ADMIN\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1015 11:03:52.401599   40096 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I1015 11:03:52.402650   40096 profile.go:143] Saving config to C:\Users\ADMIN\.minikube\profiles\minikube\config.json ...
I1015 11:03:52.504817   40096 cache.go:152] Downloading docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I1015 11:03:52.506397   40096 localpath.go:148] windows sanitize: C:\Users\ADMIN\.minikube\cache\kic\amd64\stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar -> C:\Users\ADMIN\.minikube\cache\kic\amd64\stable_v0.0.48@sha256_7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar
I1015 11:03:52.506397   40096 localpath.go:148] windows sanitize: C:\Users\ADMIN\.minikube\cache\kic\amd64\stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar -> C:\Users\ADMIN\.minikube\cache\kic\amd64\stable_v0.0.48@sha256_7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar
I1015 11:03:52.506397   40096 image.go:65] Checking for docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory
I1015 11:03:52.507502   40096 image.go:68] Found docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory, skipping pull
I1015 11:03:52.507502   40096 image.go:137] docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 exists in cache, skipping pull
I1015 11:03:52.507502   40096 cache.go:155] successfully saved docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 as a tarball
I1015 11:03:52.507502   40096 cache.go:165] Loading docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from local cache
I1015 11:03:52.507502   40096 localpath.go:148] windows sanitize: C:\Users\ADMIN\.minikube\cache\kic\amd64\stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar -> C:\Users\ADMIN\.minikube\cache\kic\amd64\stable_v0.0.48@sha256_7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar
I1015 11:04:10.044327   40096 cache.go:167] successfully loaded and using docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from cached tarball
I1015 11:04:10.044327   40096 cache.go:232] Successfully downloaded all kic artifacts
I1015 11:04:10.045934   40096 start.go:360] acquireMachinesLock for minikube: {Name:mk5f856bf614815a0986b992401e49d533f9410d Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1015 11:04:10.045934   40096 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I1015 11:04:10.045934   40096 start.go:96] Skipping create...Using existing machine configuration
I1015 11:04:10.046471   40096 fix.go:54] fixHost starting: 
I1015 11:04:10.059395   40096 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1015 11:04:10.116648   40096 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1015 11:04:10.116648   40096 fix.go:138] unexpected machine state, will restart: <nil>
I1015 11:04:10.117179   40096 out.go:252] 🔄  Restarting existing docker container for "minikube" ...
I1015 11:04:10.121984   40096 cli_runner.go:164] Run: docker start minikube
W1015 11:04:10.482494   40096 cli_runner.go:211] docker start minikube returned with exit code 1
I1015 11:04:10.486777   40096 cli_runner.go:164] Run: docker inspect minikube
I1015 11:04:10.540175   40096 errors.go:84] Postmortem inspect ("docker inspect minikube"): -- stdout --
[
    {
        "Id": "00448e4c8161de763d285ee3d2ef9eab1d7b56c9ecc3e9838421e5aa0a795d15",
        "Created": "2025-10-14T10:47:36.643695327Z",
        "Path": "/usr/local/bin/entrypoint",
        "Args": [
            "/sbin/init"
        ],
        "State": {
            "Status": "exited",
            "Running": false,
            "Paused": false,
            "Restarting": false,
            "OOMKilled": false,
            "Dead": false,
            "Pid": 0,
            "ExitCode": 128,
            "Error": "failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: unable to apply cgroup configuration: failed to write 1887: write /sys/fs/cgroup/docker/00448e4c8161de763d285ee3d2ef9eab1d7b56c9ecc3e9838421e5aa0a795d15/cgroup.procs: device or resource busy: unknown",
            "StartedAt": "2025-10-14T10:47:36.750244522Z",
            "FinishedAt": "2025-10-15T04:06:30.045302019Z"
        },
        "Image": "sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1",
        "ResolvConfPath": "/var/lib/docker/containers/00448e4c8161de763d285ee3d2ef9eab1d7b56c9ecc3e9838421e5aa0a795d15/resolv.conf",
        "HostnamePath": "/var/lib/docker/containers/00448e4c8161de763d285ee3d2ef9eab1d7b56c9ecc3e9838421e5aa0a795d15/hostname",
        "HostsPath": "/var/lib/docker/containers/00448e4c8161de763d285ee3d2ef9eab1d7b56c9ecc3e9838421e5aa0a795d15/hosts",
        "LogPath": "/var/lib/docker/containers/00448e4c8161de763d285ee3d2ef9eab1d7b56c9ecc3e9838421e5aa0a795d15/00448e4c8161de763d285ee3d2ef9eab1d7b56c9ecc3e9838421e5aa0a795d15-json.log",
        "Name": "/minikube",
        "RestartCount": 0,
        "Driver": "overlayfs",
        "Platform": "linux",
        "MountLabel": "",
        "ProcessLabel": "",
        "AppArmorProfile": "",
        "ExecIDs": null,
        "HostConfig": {
            "Binds": [
                "/lib/modules:/lib/modules:ro",
                "minikube:/var"
            ],
            "ContainerIDFile": "",
            "LogConfig": {
                "Type": "json-file",
                "Config": {}
            },
            "NetworkMode": "minikube",
            "PortBindings": {
                "22/tcp": [
                    {
                        "HostIp": "127.0.0.1",
                        "HostPort": ""
                    }
                ],
                "2376/tcp": [
                    {
                        "HostIp": "127.0.0.1",
                        "HostPort": ""
                    }
                ],
                "32443/tcp": [
                    {
                        "HostIp": "127.0.0.1",
                        "HostPort": ""
                    }
                ],
                "5000/tcp": [
                    {
                        "HostIp": "127.0.0.1",
                        "HostPort": ""
                    }
                ],
                "8443/tcp": [
                    {
                        "HostIp": "127.0.0.1",
                        "HostPort": ""
                    }
                ]
            },
            "RestartPolicy": {
                "Name": "no",
                "MaximumRetryCount": 0
            },
            "AutoRemove": false,
            "VolumeDriver": "",
            "VolumesFrom": null,
            "ConsoleSize": [
                0,
                0
            ],
            "CapAdd": null,
            "CapDrop": null,
            "CgroupnsMode": "private",
            "Dns": [],
            "DnsOptions": [],
            "DnsSearch": [],
            "ExtraHosts": null,
            "GroupAdd": null,
            "IpcMode": "private",
            "Cgroup": "",
            "Links": null,
            "OomScoreAdj": 0,
            "PidMode": "",
            "Privileged": true,
            "PublishAllPorts": false,
            "ReadonlyRootfs": false,
            "SecurityOpt": [
                "seccomp=unconfined",
                "apparmor=unconfined",
                "label=disable"
            ],
            "Tmpfs": {
                "/run": "",
                "/tmp": ""
            },
            "UTSMode": "",
            "UsernsMode": "",
            "ShmSize": 67108864,
            "Runtime": "runc",
            "Isolation": "",
            "CpuShares": 0,
            "Memory": 4089446400,
            "NanoCpus": 2000000000,
            "CgroupParent": "",
            "BlkioWeight": 0,
            "BlkioWeightDevice": [],
            "BlkioDeviceReadBps": [],
            "BlkioDeviceWriteBps": [],
            "BlkioDeviceReadIOps": [],
            "BlkioDeviceWriteIOps": [],
            "CpuPeriod": 0,
            "CpuQuota": 0,
            "CpuRealtimePeriod": 0,
            "CpuRealtimeRuntime": 0,
            "CpusetCpus": "",
            "CpusetMems": "",
            "Devices": [],
            "DeviceCgroupRules": null,
            "DeviceRequests": null,
            "MemoryReservation": 0,
            "MemorySwap": 4089446400,
            "MemorySwappiness": null,
            "OomKillDisable": null,
            "PidsLimit": null,
            "Ulimits": [],
            "CpuCount": 0,
            "CpuPercent": 0,
            "IOMaximumIOps": 0,
            "IOMaximumBandwidth": 0,
            "MaskedPaths": null,
            "ReadonlyPaths": null
        },
        "GraphDriver": {
            "Data": null,
            "Name": "overlayfs"
        },
        "Mounts": [
            {
                "Type": "bind",
                "Source": "/lib/modules",
                "Destination": "/lib/modules",
                "Mode": "ro",
                "RW": false,
                "Propagation": "rprivate"
            },
            {
                "Type": "volume",
                "Name": "minikube",
                "Source": "/var/lib/docker/volumes/minikube/_data",
                "Destination": "/var",
                "Driver": "local",
                "Mode": "z",
                "RW": true,
                "Propagation": ""
            }
        ],
        "Config": {
            "Hostname": "minikube",
            "Domainname": "",
            "User": "",
            "AttachStdin": false,
            "AttachStdout": false,
            "AttachStderr": false,
            "ExposedPorts": {
                "22/tcp": {},
                "2376/tcp": {},
                "32443/tcp": {},
                "5000/tcp": {},
                "8443/tcp": {}
            },
            "Tty": true,
            "OpenStdin": false,
            "StdinOnce": false,
            "Env": [
                "container=docker",
                "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
            ],
            "Cmd": null,
            "Image": "docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1",
            "Volumes": null,
            "WorkingDir": "/",
            "Entrypoint": [
                "/usr/local/bin/entrypoint",
                "/sbin/init"
            ],
            "OnBuild": null,
            "Labels": {
                "created_by.minikube.sigs.k8s.io": "true",
                "mode.minikube.sigs.k8s.io": "minikube",
                "name.minikube.sigs.k8s.io": "minikube",
                "role.minikube.sigs.k8s.io": ""
            },
            "StopSignal": "SIGRTMIN+3",
            "StopTimeout": 1
        },
        "NetworkSettings": {
            "Bridge": "",
            "SandboxID": "",
            "SandboxKey": "",
            "Ports": {},
            "HairpinMode": false,
            "LinkLocalIPv6Address": "",
            "LinkLocalIPv6PrefixLen": 0,
            "SecondaryIPAddresses": null,
            "SecondaryIPv6Addresses": null,
            "EndpointID": "",
            "Gateway": "",
            "GlobalIPv6Address": "",
            "GlobalIPv6PrefixLen": 0,
            "IPAddress": "",
            "IPPrefixLen": 0,
            "IPv6Gateway": "",
            "MacAddress": "",
            "Networks": {
                "minikube": {
                    "IPAMConfig": {
                        "IPv4Address": "192.168.49.2"
                    },
                    "Links": null,
                    "Aliases": null,
                    "MacAddress": "",
                    "DriverOpts": null,
                    "GwPriority": 0,
                    "NetworkID": "54b198ecad96434d9d7cb839304e07aae3567e70b268ac5df5c316afca8fbb09",
                    "EndpointID": "",
                    "Gateway": "",
                    "IPAddress": "",
                    "IPPrefixLen": 0,
                    "IPv6Gateway": "",
                    "GlobalIPv6Address": "",
                    "GlobalIPv6PrefixLen": 0,
                    "DNSNames": [
                        "minikube",
                        "00448e4c8161"
                    ]
                }
            }
        },
        "ImageManifestDescriptor": {
            "mediaType": "application/vnd.oci.image.manifest.v1+json",
            "digest": "sha256:433c461694f9d2ffe01eefef6d56beda24de10112cd7505a8f358c3ef0b8b5bc",
            "size": 483,
            "platform": {
                "architecture": "amd64",
                "os": "linux"
            }
        }
    }
]

-- /stdout --
I1015 11:04:10.544395   40096 cli_runner.go:164] Run: docker logs --timestamps --details minikube
I1015 11:04:10.601589   40096 errors.go:91] Postmortem logs ("docker logs --timestamps --details minikube"): -- stdout --
2025-10-14T10:47:37.200933356Z  + userns=
2025-10-14T10:47:37.201070395Z  + grep -Eqv '0[[:space:]]+0[[:space:]]+4294967295' /proc/self/uid_map
2025-10-14T10:47:37.204209974Z  + validate_userns
2025-10-14T10:47:37.204230233Z  + [[ -z '' ]]
2025-10-14T10:47:37.204234462Z  + return
2025-10-14T10:47:37.204238009Z  + configure_containerd
2025-10-14T10:47:37.204241075Z  + local snapshotter=
2025-10-14T10:47:37.204350469Z  + [[ -n '' ]]
2025-10-14T10:47:37.204363193Z  + [[ -z '' ]]
2025-10-14T10:47:37.205851704Z  ++ stat -f -c %T /kind
2025-10-14T10:47:37.209494663Z  + container_filesystem=overlayfs
2025-10-14T10:47:37.209566414Z  + [[ overlayfs == \z\f\s ]]
2025-10-14T10:47:37.209572065Z  + [[ -n '' ]]
2025-10-14T10:47:37.209767187Z  + configure_proxy
2025-10-14T10:47:37.209777026Z  + mkdir -p /etc/systemd/system.conf.d/
2025-10-14T10:47:37.214564548Z  + [[ ! -z '' ]]
2025-10-14T10:47:37.214587783Z  + cat
2025-10-14T10:47:37.216630728Z  + fix_mount
2025-10-14T10:47:37.216646990Z  + echo 'INFO: ensuring we can execute mount/umount even with userns-remap'
2025-10-14T10:47:37.216656559Z  INFO: ensuring we can execute mount/umount even with userns-remap
2025-10-14T10:47:37.217227918Z  ++ which mount
2025-10-14T10:47:37.219803512Z  ++ which umount
2025-10-14T10:47:37.221533701Z  + chown root:root /usr/bin/mount /usr/bin/umount
2025-10-14T10:47:37.230007926Z  ++ which mount
2025-10-14T10:47:37.232274881Z  ++ which umount
2025-10-14T10:47:37.233907784Z  + chmod -s /usr/bin/mount /usr/bin/umount
2025-10-14T10:47:37.237210081Z  +++ which mount
2025-10-14T10:47:37.238892141Z  ++ stat -f -c %T /usr/bin/mount
2025-10-14T10:47:37.240773790Z  + [[ overlayfs == \a\u\f\s ]]
2025-10-14T10:47:37.240789591Z  + echo 'INFO: remounting /sys read-only'
2025-10-14T10:47:37.240794511Z  INFO: remounting /sys read-only
2025-10-14T10:47:37.240798779Z  + mount -o remount,ro /sys
2025-10-14T10:47:37.244128520Z  + echo 'INFO: making mounts shared'
2025-10-14T10:47:37.244154962Z  INFO: making mounts shared
2025-10-14T10:47:37.244158489Z  + mount --make-rshared /
2025-10-14T10:47:37.247152141Z  + retryable_fix_cgroup
2025-10-14T10:47:37.247822545Z  ++ seq 0 10
2025-10-14T10:47:37.249355356Z  + for i in $(seq 0 10)
2025-10-14T10:47:37.249372139Z  + fix_cgroup
2025-10-14T10:47:37.249375937Z  + [[ -f /sys/fs/cgroup/cgroup.controllers ]]
2025-10-14T10:47:37.249378461Z  + echo 'INFO: detected cgroup v2'
2025-10-14T10:47:37.249380856Z  INFO: detected cgroup v2
2025-10-14T10:47:37.249409432Z  + return
2025-10-14T10:47:37.249417438Z  + return
2025-10-14T10:47:37.249556530Z  + fix_machine_id
2025-10-14T10:47:37.249567832Z  + echo 'INFO: clearing and regenerating /etc/machine-id'
2025-10-14T10:47:37.249574846Z  INFO: clearing and regenerating /etc/machine-id
2025-10-14T10:47:37.249755901Z  + rm -f /etc/machine-id
2025-10-14T10:47:37.252479133Z  + systemd-machine-id-setup
2025-10-14T10:47:37.258536555Z  Initializing machine ID from random generator.
2025-10-14T10:47:37.261791290Z  + fix_product_name
2025-10-14T10:47:37.261809966Z  + [[ -f /sys/class/dmi/id/product_name ]]
2025-10-14T10:47:37.261814185Z  + fix_product_uuid
2025-10-14T10:47:37.261817631Z  + [[ ! -f /kind/product_uuid ]]
2025-10-14T10:47:37.261866016Z  + cat /proc/sys/kernel/random/uuid
2025-10-14T10:47:37.265051825Z  + [[ -f /sys/class/dmi/id/product_uuid ]]
2025-10-14T10:47:37.265082315Z  + [[ -f /sys/devices/virtual/dmi/id/product_uuid ]]
2025-10-14T10:47:37.265343496Z  + select_iptables
2025-10-14T10:47:37.265353045Z  + local mode num_legacy_lines num_nft_lines
2025-10-14T10:47:37.267142910Z  ++ grep -c '^-'
2025-10-14T10:47:37.272500322Z  ++ true
2025-10-14T10:47:37.272851590Z  + num_legacy_lines=0
2025-10-14T10:47:37.275046299Z  ++ grep -c '^-'
2025-10-14T10:47:37.287821057Z  + num_nft_lines=6
2025-10-14T10:47:37.287857809Z  + '[' 0 -ge 6 ']'
2025-10-14T10:47:37.287862508Z  + mode=nft
2025-10-14T10:47:37.287866466Z  + echo 'INFO: setting iptables to detected mode: nft'
2025-10-14T10:47:37.287870333Z  INFO: setting iptables to detected mode: nft
2025-10-14T10:47:37.287877447Z  + update-alternatives --set iptables /usr/sbin/iptables-nft
2025-10-14T10:47:37.287956953Z  + echo 'retryable update-alternatives: --set iptables /usr/sbin/iptables-nft'
2025-10-14T10:47:37.287966421Z  + local 'args=--set iptables /usr/sbin/iptables-nft'
2025-10-14T10:47:37.288774755Z  ++ seq 0 15
2025-10-14T10:47:37.290543218Z  + for i in $(seq 0 15)
2025-10-14T10:47:37.290564800Z  + /usr/bin/update-alternatives --set iptables /usr/sbin/iptables-nft
2025-10-14T10:47:37.296167883Z  + return
2025-10-14T10:47:37.296210587Z  + update-alternatives --set ip6tables /usr/sbin/ip6tables-nft
2025-10-14T10:47:37.296218813Z  + echo 'retryable update-alternatives: --set ip6tables /usr/sbin/ip6tables-nft'
2025-10-14T10:47:37.296223242Z  + local 'args=--set ip6tables /usr/sbin/ip6tables-nft'
2025-10-14T10:47:37.297279059Z  ++ seq 0 15
2025-10-14T10:47:37.298957041Z  + for i in $(seq 0 15)
2025-10-14T10:47:37.299005677Z  + /usr/bin/update-alternatives --set ip6tables /usr/sbin/ip6tables-nft
2025-10-14T10:47:37.303797195Z  + return
2025-10-14T10:47:37.303886309Z  + enable_network_magic
2025-10-14T10:47:37.303894525Z  + local docker_embedded_dns_ip=127.0.0.11
2025-10-14T10:47:37.303987216Z  + local docker_host_ip
2025-10-14T10:47:37.306399010Z  ++ cut '-d ' -f1
2025-10-14T10:47:37.306414229Z  ++ head -n1 /dev/fd/63
2025-10-14T10:47:37.306728093Z  +++ timeout 5 getent ahostsv4 host.docker.internal
2025-10-14T10:47:37.321936478Z  + docker_host_ip=192.168.65.254
2025-10-14T10:47:37.321973721Z  + [[ -z 192.168.65.254 ]]
2025-10-14T10:47:37.321981536Z  + [[ 192.168.65.254 =~ ^127\.[0-9]+\.[0-9]+\.[0-9]+$ ]]
2025-10-14T10:47:37.322910996Z  + iptables-save
2025-10-14T10:47:37.324079334Z  + iptables-restore
2025-10-14T10:47:37.330899942Z  + sed -e 's/-d 127.0.0.11/-d 192.168.65.254/g' -e 's/-A OUTPUT \(.*\) -j DOCKER_OUTPUT/\0\n-A PREROUTING \1 -j DOCKER_OUTPUT/' -e 's/--to-source :53/--to-source 192.168.65.254:53/g' -e 's/p -j DNAT --to-destination 127.0.0.11/p --dport 53 -j DNAT --to-destination 127.0.0.11/g'
2025-10-14T10:47:37.371779380Z  + cp /etc/resolv.conf /etc/resolv.conf.original
2025-10-14T10:47:37.375538292Z  ++ sed -e s/127.0.0.11/192.168.65.254/g /etc/resolv.conf.original
2025-10-14T10:47:37.378272531Z  + replaced='# Generated by Docker Engine.
2025-10-14T10:47:37.378296368Z  # This file can be edited; Docker Engine will not make further changes once it
2025-10-14T10:47:37.378306728Z  # has been modified.
2025-10-14T10:47:37.378312189Z  
2025-10-14T10:47:37.378316588Z  nameserver 192.168.65.254
2025-10-14T10:47:37.378320686Z  options ndots:0
2025-10-14T10:47:37.378324513Z  
2025-10-14T10:47:37.378328321Z  # Based on host file: '\''/etc/resolv.conf'\'' (internal resolver)
2025-10-14T10:47:37.378332068Z  # ExtServers: [host(192.168.65.7)]
2025-10-14T10:47:37.378335765Z  # Overrides: []
2025-10-14T10:47:37.378340004Z  # Option ndots from: internal'
2025-10-14T10:47:37.378345154Z  + [[ '' == '' ]]
2025-10-14T10:47:37.378353280Z  + echo '# Generated by Docker Engine.
2025-10-14T10:47:37.378357638Z  # This file can be edited; Docker Engine will not make further changes once it
2025-10-14T10:47:37.378361856Z  # has been modified.
2025-10-14T10:47:37.378384080Z  
2025-10-14T10:47:37.378402506Z  nameserver 192.168.65.254
2025-10-14T10:47:37.378407145Z  options ndots:0
2025-10-14T10:47:37.378410983Z  
2025-10-14T10:47:37.378414830Z  # Based on host file: '\''/etc/resolv.conf'\'' (internal resolver)
2025-10-14T10:47:37.378419179Z  # ExtServers: [host(192.168.65.7)]
2025-10-14T10:47:37.378425421Z  # Overrides: []
2025-10-14T10:47:37.378445240Z  # Option ndots from: internal'
2025-10-14T10:47:37.378698611Z  + files_to_update=('/etc/kubernetes/manifests/etcd.yaml' '/etc/kubernetes/manifests/kube-apiserver.yaml' '/etc/kubernetes/manifests/kube-controller-manager.yaml' '/etc/kubernetes/manifests/kube-scheduler.yaml' '/etc/kubernetes/controller-manager.conf' '/etc/kubernetes/scheduler.conf' '/kind/kubeadm.conf' '/var/lib/kubelet/kubeadm-flags.env')
2025-10-14T10:47:37.378749811Z  + local files_to_update
2025-10-14T10:47:37.378758077Z  + local should_fix_certificate=false
2025-10-14T10:47:37.381276840Z  ++ cut '-d ' -f1
2025-10-14T10:47:37.381294545Z  ++ head -n1 /dev/fd/63
2025-10-14T10:47:37.382195599Z  ++++ hostname
2025-10-14T10:47:37.383989904Z  +++ timeout 5 getent ahostsv4 minikube
2025-10-14T10:47:37.389894351Z  + curr_ipv4=192.168.49.2
2025-10-14T10:47:37.389918589Z  + echo 'INFO: Detected IPv4 address: 192.168.49.2'
2025-10-14T10:47:37.389924470Z  INFO: Detected IPv4 address: 192.168.49.2
2025-10-14T10:47:37.389928949Z  + '[' -f /kind/old-ipv4 ']'
2025-10-14T10:47:37.389968697Z  + [[ -n 192.168.49.2 ]]
2025-10-14T10:47:37.389975941Z  + echo -n 192.168.49.2
2025-10-14T10:47:37.392654406Z  ++ head -n1 /dev/fd/63
2025-10-14T10:47:37.392681780Z  ++ cut '-d ' -f1
2025-10-14T10:47:37.393395593Z  ++++ hostname
2025-10-14T10:47:37.395136191Z  +++ timeout 5 getent ahostsv6 minikube
2025-10-14T10:47:37.400057374Z  + curr_ipv6=
2025-10-14T10:47:37.400091702Z  + echo 'INFO: Detected IPv6 address: '
2025-10-14T10:47:37.400100910Z  INFO: Detected IPv6 address: 
2025-10-14T10:47:37.400122662Z  + '[' -f /kind/old-ipv6 ']'
2025-10-14T10:47:37.400128394Z  + [[ -n '' ]]
2025-10-14T10:47:37.400133003Z  + false
2025-10-14T10:47:37.401129424Z  ++ uname -a
2025-10-14T10:47:37.402749186Z  + echo 'entrypoint completed: Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux'
2025-10-14T10:47:37.402764616Z  entrypoint completed: Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
2025-10-14T10:47:37.402769215Z  + exec /sbin/init
2025-10-14T10:47:37.427619157Z  Inserted module 'autofs4'
2025-10-14T10:47:37.430058410Z  systemd 249.11-0ubuntu3.16 running in system mode (+PAM +AUDIT +SELINUX +APPARMOR +IMA +SMACK +SECCOMP +GCRYPT +GNUTLS +OPENSSL +ACL +BLKID +CURL +ELFUTILS +FIDO2 +IDN2 -IDN +IPTC +KMOD +LIBCRYPTSETUP +LIBFDISK +PCRE2 -PWQUALITY -P11KIT -QRENCODE +BZIP2 +LZ4 +XZ +ZLIB +ZSTD -XKBCOMMON +UTMP +SYSVINIT default-hierarchy=unified)
2025-10-14T10:47:37.430076726Z  Detected virtualization wsl.
2025-10-14T10:47:37.430082216Z  Detected architecture x86-64.
2025-10-14T10:47:37.430442312Z  
2025-10-14T10:47:37.430484194Z  Welcome to [1mUbuntu 22.04.5 LTS[0m!
2025-10-14T10:47:37.430489875Z  
2025-10-14T10:47:37.533249692Z  Queued start job for default target Graphical Interface.
2025-10-14T10:47:37.553088785Z  [[0;32m  OK  [0m] Created slice [0;1;39mSlice /system/modprobe[0m.
2025-10-14T10:47:37.553127290Z  [[0;32m  OK  [0m] Started [0;1;39mDispatch Password …ts to Console Directory Watch[0m.
2025-10-14T10:47:37.553512480Z  [[0;1;38;5;185mUNSUPP[0m] Starting of [0;1;39mArbitrary Exec…m Automount Point[0m unsupported.
2025-10-14T10:47:37.553529123Z  [[0;32m  OK  [0m] Reached target [0;1;39mLocal Encrypted Volumes[0m.
2025-10-14T10:47:37.553535094Z  [[0;32m  OK  [0m] Reached target [0;1;39mNetwork is Online[0m.
2025-10-14T10:47:37.553540014Z  [[0;32m  OK  [0m] Reached target [0;1;39mPath Units[0m.
2025-10-14T10:47:37.553546607Z  [[0;32m  OK  [0m] Reached target [0;1;39mSlice Units[0m.
2025-10-14T10:47:37.553554562Z  [[0;32m  OK  [0m] Reached target [0;1;39mSwaps[0m.
2025-10-14T10:47:37.553559883Z  [[0;32m  OK  [0m] Reached target [0;1;39mLocal Verity Protected Volumes[0m.
2025-10-14T10:47:37.554034838Z  [[0;32m  OK  [0m] Listening on [0;1;39mJournal Audit Socket[0m.
2025-10-14T10:47:37.554130055Z  [[0;32m  OK  [0m] Listening on [0;1;39mJournal Socket (/dev/log)[0m.
2025-10-14T10:47:37.554766411Z  [[0;32m  OK  [0m] Listening on [0;1;39mJournal Socket[0m.
2025-10-14T10:47:37.556971579Z           Mounting [0;1;39mHuge Pages File System[0m...
2025-10-14T10:47:37.559445274Z           Mounting [0;1;39mKernel Debug File System[0m...
2025-10-14T10:47:37.562719391Z           Mounting [0;1;39mKernel Trace File System[0m...
2025-10-14T10:47:37.569494935Z           Starting [0;1;39mJournal Service[0m...
2025-10-14T10:47:37.573862868Z           Starting [0;1;39mCreate List of Static Device Nodes[0m...
2025-10-14T10:47:37.576825761Z           Starting [0;1;39mLoad Kernel Module configfs[0m...
2025-10-14T10:47:37.579306519Z           Starting [0;1;39mLoad Kernel Module fuse[0m...
2025-10-14T10:47:37.582226658Z           Starting [0;1;39mRemount Root and Kernel File Systems[0m...
2025-10-14T10:47:37.584821650Z           Starting [0;1;39mApply Kernel Variables[0m...
2025-10-14T10:47:37.588016337Z  [[0;32m  OK  [0m] Mounted [0;1;39mHuge Pages File System[0m.
2025-10-14T10:47:37.588348316Z  [[0;32m  OK  [0m] Mounted [0;1;39mKernel Debug File System[0m.
2025-10-14T10:47:37.588707941Z  [[0;32m  OK  [0m] Mounted [0;1;39mKernel Trace File System[0m.
2025-10-14T10:47:37.590023723Z  [[0;32m  OK  [0m] Finished [0;1;39mCreate List of Static Device Nodes[0m.
2025-10-14T10:47:37.590961459Z  modprobe@configfs.service: Deactivated successfully.
2025-10-14T10:47:37.591649848Z  [[0;32m  OK  [0m] Finished [0;1;39mLoad Kernel Module configfs[0m.
2025-10-14T10:47:37.592196252Z  modprobe@fuse.service: Deactivated successfully.
2025-10-14T10:47:37.592938987Z  [[0;32m  OK  [0m] Finished [0;1;39mLoad Kernel Module fuse[0m.
2025-10-14T10:47:37.593900510Z  [[0;32m  OK  [0m] Finished [0;1;39mRemount Root and Kernel File Systems[0m.
2025-10-14T10:47:37.596642139Z           Mounting [0;1;39mFUSE Control File System[0m...
2025-10-14T10:47:37.599652525Z           Starting [0;1;39mCreate System Users[0m...
2025-10-14T10:47:37.602577824Z           Starting [0;1;39mRecord System Boot/Shutdown in UTMP[0m...
2025-10-14T10:47:37.604107329Z  [[0;32m  OK  [0m] Started [0;1;39mJournal Service[0m.
2025-10-14T10:47:37.605704447Z  [[0;32m  OK  [0m] Finished [0;1;39mApply Kernel Variables[0m.
2025-10-14T10:47:37.606109079Z  [[0;32m  OK  [0m] Mounted [0;1;39mFUSE Control File System[0m.
2025-10-14T10:47:37.608750448Z           Starting [0;1;39mFlush Journal to Persistent Storage[0m...
2025-10-14T10:47:37.613081585Z  [[0;32m  OK  [0m] Finished [0;1;39mRecord System Boot/Shutdown in UTMP[0m.
2025-10-14T10:47:37.613896410Z  [[0;32m  OK  [0m] Finished [0;1;39mCreate System Users[0m.
2025-10-14T10:47:37.620894996Z           Starting [0;1;39mCreate Static Device Nodes in /dev[0m...
2025-10-14T10:47:37.628227335Z  [[0;32m  OK  [0m] Finished [0;1;39mFlush Journal to Persistent Storage[0m.
2025-10-14T10:47:37.632016025Z  [[0;32m  OK  [0m] Finished [0;1;39mCreate Static Device Nodes in /dev[0m.
2025-10-14T10:47:37.632236226Z  [[0;32m  OK  [0m] Reached target [0;1;39mPreparation for Local File Systems[0m.
2025-10-14T10:47:37.632247027Z  [[0;32m  OK  [0m] Reached target [0;1;39mLocal File Systems[0m.
2025-10-14T10:47:37.632742628Z  [[0;32m  OK  [0m] Reached target [0;1;39mSystem Initialization[0m.
2025-10-14T10:47:37.633878457Z  [[0;32m  OK  [0m] Started [0;1;39mPodman auto-update timer[0m.
2025-10-14T10:47:37.633891964Z  [[0;32m  OK  [0m] Started [0;1;39mDaily Cleanup of Temporary Directories[0m.
2025-10-14T10:47:37.634117521Z  [[0;32m  OK  [0m] Reached target [0;1;39mTimer Units[0m.
2025-10-14T10:47:37.634576409Z  [[0;32m  OK  [0m] Listening on [0;1;39mBuildKit[0m.
2025-10-14T10:47:37.636691942Z           Starting [0;1;39mDocker Socket for the API[0m...
2025-10-14T10:47:37.638621045Z           Starting [0;1;39mPodman API Socket[0m...
2025-10-14T10:47:37.639148984Z  [[0;32m  OK  [0m] Listening on [0;1;39mDocker Socket for the API[0m.
2025-10-14T10:47:37.639962783Z  [[0;32m  OK  [0m] Listening on [0;1;39mPodman API Socket[0m.
2025-10-14T10:47:37.639978464Z  [[0;32m  OK  [0m] Reached target [0;1;39mSocket Units[0m.
2025-10-14T10:47:37.640261818Z  [[0;32m  OK  [0m] Reached target [0;1;39mBasic System[0m.
2025-10-14T10:47:37.642106045Z           Starting [0;1;39mcontainerd container runtime[0m...
2025-10-14T10:47:37.644249743Z           Starting [0;1;39mminikube automount[0m...
2025-10-14T10:47:37.646706064Z           Starting [0;1;39mPodman auto-update service[0m...
2025-10-14T10:47:37.649667734Z           Starting [0;1;39mPodman Start All …estart Policy Set To Always[0m...
2025-10-14T10:47:37.652413110Z           Starting [0;1;39mPodman API Service[0m...
2025-10-14T10:47:37.658824767Z           Starting [0;1;39mOpenBSD Secure Shell server[0m...
2025-10-14T10:47:37.662636662Z  [[0;32m  OK  [0m] Started [0;1;39mPodman API Service[0m.
2025-10-14T10:47:37.745190163Z  [[0;32m  OK  [0m] Started [0;1;39mOpenBSD Secure Shell server[0m.
2025-10-14T10:47:37.746102871Z  [[0;32m  OK  [0m] Finished [0;1;39mminikube automount[0m.
2025-10-14T10:47:37.885661913Z  [[0;32m  OK  [0m] Started [0;1;39mcontainerd container runtime[0m.
2025-10-14T10:47:37.919731434Z           Starting [0;1;39mDocker Application Container Engine[0m...
2025-10-14T10:47:38.153310251Z  [[0;32m  OK  [0m] Finished [0;1;39mPodman Start All … Restart Policy Set To Always[0m.
2025-10-14T10:47:38.240210705Z  [[0;32m  OK  [0m] Finished [0;1;39mPodman auto-update service[0m.
2025-10-14T10:47:38.594766423Z  [[0;32m  OK  [0m] Started [0;1;39mDocker Application Container Engine[0m.
2025-10-14T10:47:38.595005953Z  [[0;32m  OK  [0m] Reached target [0;1;39mMulti-User System[0m.
2025-10-14T10:47:38.595266479Z  [[0;32m  OK  [0m] Reached target [0;1;39mGraphical Interface[0m.
2025-10-14T10:47:38.639382008Z           Starting [0;1;39mRecord Runlevel Change in UTMP[0m...
2025-10-14T10:47:38.649076484Z  [[0;32m  OK  [0m] Finished [0;1;39mRecord Runlevel Change in UTMP[0m.

-- /stdout --
I1015 11:04:10.605776   40096 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1015 11:04:10.841527   40096 info.go:266] docker info: {ID:b24ccc47-792f-41a6-a448-1ca3602492c1 Containers:3 ContainersRunning:2 ContainersPaused:0 ContainersStopped:1 Images:6 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:50 OomKillDisable:false NGoroutines:109 SystemTime:2025-10-15 05:34:10.823176478 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:7959953408 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.39] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.44] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Users\ADMIN\.docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-mcp.exe] ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.23.0] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.42] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I1015 11:04:10.841527   40096 errors.go:98] postmortem docker info: {ID:b24ccc47-792f-41a6-a448-1ca3602492c1 Containers:3 ContainersRunning:2 ContainersPaused:0 ContainersStopped:1 Images:6 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:50 OomKillDisable:false NGoroutines:109 SystemTime:2025-10-15 05:34:10.823176478 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:7959953408 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.39] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.44] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Users\ADMIN\.docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-mcp.exe] ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.23.0] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.42] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I1015 11:04:10.845814   40096 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I1015 11:04:10.845814   40096 cli_runner.go:164] Run: docker network inspect minikube
I1015 11:04:10.896910   40096 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[
    {
        "Name": "minikube",
        "Id": "54b198ecad96434d9d7cb839304e07aae3567e70b268ac5df5c316afca8fbb09",
        "Created": "2025-10-14T10:47:25.097578815Z",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv4": true,
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": {},
            "Config": [
                {
                    "Subnet": "192.168.49.0/24",
                    "Gateway": "192.168.49.1"
                }
            ]
        },
        "Internal": false,
        "Attachable": false,
        "Ingress": false,
        "ConfigFrom": {
            "Network": ""
        },
        "ConfigOnly": false,
        "Containers": {},
        "Options": {
            "--icc": "",
            "--ip-masq": "",
            "com.docker.network.driver.mtu": "1500",
            "com.docker.network.enable_ipv4": "true",
            "com.docker.network.enable_ipv6": "false"
        },
        "Labels": {
            "created_by.minikube.sigs.k8s.io": "true",
            "name.minikube.sigs.k8s.io": "minikube"
        }
    }
]

-- /stdout --
I1015 11:04:10.901123   40096 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1015 11:04:11.135753   40096 info.go:266] docker info: {ID:b24ccc47-792f-41a6-a448-1ca3602492c1 Containers:3 ContainersRunning:2 ContainersPaused:0 ContainersStopped:1 Images:6 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:50 OomKillDisable:false NGoroutines:109 SystemTime:2025-10-15 05:34:11.118981473 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:7959953408 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.39] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.44] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Users\ADMIN\.docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-mcp.exe] ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.23.0] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.42] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I1015 11:04:11.143625   40096 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1015 11:04:11.149385   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
W1015 11:04:11.201715   40096 cli_runner.go:211] docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube returned with exit code 1
I1015 11:04:11.202255   40096 retry.go:31] will retry after 365.136383ms: new client: new client: Error creating new ssh host from driver: Error getting ssh port for driver: get ssh host-port: unable to inspect a not running container to get SSH port
I1015 11:04:11.572965   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
W1015 11:04:11.626613   40096 cli_runner.go:211] docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube returned with exit code 1
I1015 11:04:11.626613   40096 retry.go:31] will retry after 558.207128ms: new client: new client: Error creating new ssh host from driver: Error getting ssh port for driver: get ssh host-port: unable to inspect a not running container to get SSH port
I1015 11:04:12.190736   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
W1015 11:04:12.245696   40096 cli_runner.go:211] docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube returned with exit code 1
I1015 11:04:12.245696   40096 retry.go:31] will retry after 314.58528ms: new client: new client: Error creating new ssh host from driver: Error getting ssh port for driver: get ssh host-port: unable to inspect a not running container to get SSH port
I1015 11:04:12.565350   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
W1015 11:04:12.620302   40096 cli_runner.go:211] docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube returned with exit code 1
W1015 11:04:12.620302   40096 start.go:268] error running df -h /var: NewSession: new client: new client: Error creating new ssh host from driver: Error getting ssh port for driver: get ssh host-port: unable to inspect a not running container to get SSH port
W1015 11:04:12.621380   40096 start.go:235] error getting percentage of /var that is free: NewSession: new client: new client: Error creating new ssh host from driver: Error getting ssh port for driver: get ssh host-port: unable to inspect a not running container to get SSH port
I1015 11:04:12.628716   40096 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1015 11:04:12.632918   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
W1015 11:04:12.688005   40096 cli_runner.go:211] docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube returned with exit code 1
I1015 11:04:12.688005   40096 retry.go:31] will retry after 340.037859ms: new client: new client: Error creating new ssh host from driver: Error getting ssh port for driver: get ssh host-port: unable to inspect a not running container to get SSH port
I1015 11:04:13.033620   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
W1015 11:04:13.090384   40096 cli_runner.go:211] docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube returned with exit code 1
I1015 11:04:13.090384   40096 retry.go:31] will retry after 258.477014ms: new client: new client: Error creating new ssh host from driver: Error getting ssh port for driver: get ssh host-port: unable to inspect a not running container to get SSH port
I1015 11:04:13.353939   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
W1015 11:04:13.415391   40096 cli_runner.go:211] docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube returned with exit code 1
I1015 11:04:13.415391   40096 retry.go:31] will retry after 512.045685ms: new client: new client: Error creating new ssh host from driver: Error getting ssh port for driver: get ssh host-port: unable to inspect a not running container to get SSH port
I1015 11:04:13.931772   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
W1015 11:04:13.986324   40096 cli_runner.go:211] docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube returned with exit code 1
W1015 11:04:13.986324   40096 start.go:283] error running df -BG /var: NewSession: new client: new client: Error creating new ssh host from driver: Error getting ssh port for driver: get ssh host-port: unable to inspect a not running container to get SSH port
W1015 11:04:13.986324   40096 start.go:240] error getting GiB of /var that is available: NewSession: new client: new client: Error creating new ssh host from driver: Error getting ssh port for driver: get ssh host-port: unable to inspect a not running container to get SSH port
I1015 11:04:13.986324   40096 fix.go:56] duration metric: took 3.9403903s for fixHost
I1015 11:04:13.986324   40096 start.go:83] releasing machines lock for "minikube", held for 3.9403903s
W1015 11:04:13.986857   40096 start.go:714] error starting host: driver start: start: docker start minikube: exit status 1
stdout:

stderr:
Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: unable to apply cgroup configuration: failed to write 1887: write /sys/fs/cgroup/docker/00448e4c8161de763d285ee3d2ef9eab1d7b56c9ecc3e9838421e5aa0a795d15/cgroup.procs: device or resource busy: unknown
Error: failed to start containers: minikube
W1015 11:04:13.987403   40096 out.go:285] 🤦  StartHost failed, but will try again: driver start: start: docker start minikube: exit status 1
stdout:

stderr:
Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: unable to apply cgroup configuration: failed to write 1887: write /sys/fs/cgroup/docker/00448e4c8161de763d285ee3d2ef9eab1d7b56c9ecc3e9838421e5aa0a795d15/cgroup.procs: device or resource busy: unknown
Error: failed to start containers: minikube

I1015 11:04:13.988011   40096 start.go:729] Will try again in 5 seconds ...
I1015 11:04:18.988929   40096 start.go:360] acquireMachinesLock for minikube: {Name:mk5f856bf614815a0986b992401e49d533f9410d Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1015 11:04:18.989629   40096 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I1015 11:04:18.989629   40096 start.go:96] Skipping create...Using existing machine configuration
I1015 11:04:18.989629   40096 fix.go:54] fixHost starting: 
I1015 11:04:18.999231   40096 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1015 11:04:19.056486   40096 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1015 11:04:19.056486   40096 fix.go:138] unexpected machine state, will restart: <nil>
I1015 11:04:19.057541   40096 out.go:252] 🔄  Restarting existing docker container for "minikube" ...
I1015 11:04:19.062959   40096 cli_runner.go:164] Run: docker start minikube
I1015 11:04:19.864305   40096 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1015 11:04:19.956849   40096 kic.go:430] container "minikube" state is running.
I1015 11:04:19.969402   40096 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1015 11:04:20.060607   40096 profile.go:143] Saving config to C:\Users\ADMIN\.minikube\profiles\minikube\config.json ...
I1015 11:04:20.063310   40096 machine.go:93] provisionDockerMachine start ...
I1015 11:04:20.074815   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 11:04:20.168132   40096 main.go:141] libmachine: Using SSH client type: native
I1015 11:04:20.186840   40096 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x13417c0] 0x1344300 <nil>  [] 0s} 127.0.0.1 53974 <nil> <nil>}
I1015 11:04:20.186840   40096 main.go:141] libmachine: About to run SSH command:
hostname
I1015 11:04:20.190276   40096 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1015 11:04:23.349859   40096 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1015 11:04:23.350933   40096 ubuntu.go:182] provisioning hostname "minikube"
I1015 11:04:23.355760   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 11:04:23.412410   40096 main.go:141] libmachine: Using SSH client type: native
I1015 11:04:23.412410   40096 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x13417c0] 0x1344300 <nil>  [] 0s} 127.0.0.1 53974 <nil> <nil>}
I1015 11:04:23.412410   40096 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1015 11:04:23.595578   40096 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1015 11:04:23.601978   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 11:04:23.660664   40096 main.go:141] libmachine: Using SSH client type: native
I1015 11:04:23.661205   40096 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x13417c0] 0x1344300 <nil>  [] 0s} 127.0.0.1 53974 <nil> <nil>}
I1015 11:04:23.661205   40096 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1015 11:04:23.808325   40096 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1015 11:04:23.808856   40096 ubuntu.go:188] set auth options {CertDir:C:\Users\ADMIN\.minikube CaCertPath:C:\Users\ADMIN\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\ADMIN\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\ADMIN\.minikube\machines\server.pem ServerKeyPath:C:\Users\ADMIN\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\ADMIN\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\ADMIN\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\ADMIN\.minikube}
I1015 11:04:23.808856   40096 ubuntu.go:190] setting up certificates
I1015 11:04:23.809390   40096 provision.go:84] configureAuth start
I1015 11:04:23.813715   40096 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1015 11:04:23.866883   40096 provision.go:143] copyHostCerts
I1015 11:04:23.886546   40096 exec_runner.go:144] found C:\Users\ADMIN\.minikube/ca.pem, removing ...
I1015 11:04:23.887120   40096 exec_runner.go:203] rm: C:\Users\ADMIN\.minikube\ca.pem
I1015 11:04:23.887120   40096 exec_runner.go:151] cp: C:\Users\ADMIN\.minikube\certs\ca.pem --> C:\Users\ADMIN\.minikube/ca.pem (1074 bytes)
I1015 11:04:23.903619   40096 exec_runner.go:144] found C:\Users\ADMIN\.minikube/cert.pem, removing ...
I1015 11:04:23.903619   40096 exec_runner.go:203] rm: C:\Users\ADMIN\.minikube\cert.pem
I1015 11:04:23.904127   40096 exec_runner.go:151] cp: C:\Users\ADMIN\.minikube\certs\cert.pem --> C:\Users\ADMIN\.minikube/cert.pem (1119 bytes)
I1015 11:04:23.919852   40096 exec_runner.go:144] found C:\Users\ADMIN\.minikube/key.pem, removing ...
I1015 11:04:23.919852   40096 exec_runner.go:203] rm: C:\Users\ADMIN\.minikube\key.pem
I1015 11:04:23.920365   40096 exec_runner.go:151] cp: C:\Users\ADMIN\.minikube\certs\key.pem --> C:\Users\ADMIN\.minikube/key.pem (1675 bytes)
I1015 11:04:23.920898   40096 provision.go:117] generating server cert: C:\Users\ADMIN\.minikube\machines\server.pem ca-key=C:\Users\ADMIN\.minikube\certs\ca.pem private-key=C:\Users\ADMIN\.minikube\certs\ca-key.pem org=ADMIN.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1015 11:04:24.191554   40096 provision.go:177] copyRemoteCerts
I1015 11:04:24.192650   40096 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1015 11:04:24.197626   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 11:04:24.257196   40096 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53974 SSHKeyPath:C:\Users\ADMIN\.minikube\machines\minikube\id_rsa Username:docker}
I1015 11:04:24.372380   40096 ssh_runner.go:362] scp C:\Users\ADMIN\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I1015 11:04:24.418142   40096 ssh_runner.go:362] scp C:\Users\ADMIN\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1015 11:04:24.458287   40096 ssh_runner.go:362] scp C:\Users\ADMIN\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1015 11:04:24.498010   40096 provision.go:87] duration metric: took 688.6197ms to configureAuth
I1015 11:04:24.498010   40096 ubuntu.go:206] setting minikube options for container-runtime
I1015 11:04:24.498545   40096 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1015 11:04:24.503394   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 11:04:24.557326   40096 main.go:141] libmachine: Using SSH client type: native
I1015 11:04:24.557850   40096 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x13417c0] 0x1344300 <nil>  [] 0s} 127.0.0.1 53974 <nil> <nil>}
I1015 11:04:24.557850   40096 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1015 11:04:24.708611   40096 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1015 11:04:24.708611   40096 ubuntu.go:71] root file system type: overlay
I1015 11:04:24.709146   40096 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1015 11:04:24.714105   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 11:04:24.770726   40096 main.go:141] libmachine: Using SSH client type: native
I1015 11:04:24.771261   40096 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x13417c0] 0x1344300 <nil>  [] 0s} 127.0.0.1 53974 <nil> <nil>}
I1015 11:04:24.771261   40096 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1015 11:04:24.942262   40096 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I1015 11:04:24.948337   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 11:04:25.004857   40096 main.go:141] libmachine: Using SSH client type: native
I1015 11:04:25.005405   40096 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x13417c0] 0x1344300 <nil>  [] 0s} 127.0.0.1 53974 <nil> <nil>}
I1015 11:04:25.005405   40096 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1015 11:04:25.164965   40096 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1015 11:04:25.164965   40096 machine.go:96] duration metric: took 5.1016553s to provisionDockerMachine
I1015 11:04:25.165475   40096 start.go:293] postStartSetup for "minikube" (driver="docker")
I1015 11:04:25.165475   40096 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1015 11:04:25.167065   40096 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1015 11:04:25.171372   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 11:04:25.225490   40096 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53974 SSHKeyPath:C:\Users\ADMIN\.minikube\machines\minikube\id_rsa Username:docker}
I1015 11:04:25.347246   40096 ssh_runner.go:195] Run: cat /etc/os-release
I1015 11:04:25.354281   40096 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1015 11:04:25.354281   40096 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1015 11:04:25.354281   40096 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1015 11:04:25.354281   40096 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I1015 11:04:25.354825   40096 filesync.go:126] Scanning C:\Users\ADMIN\.minikube\addons for local assets ...
I1015 11:04:25.355429   40096 filesync.go:126] Scanning C:\Users\ADMIN\.minikube\files for local assets ...
I1015 11:04:25.355429   40096 start.go:296] duration metric: took 189.9538ms for postStartSetup
I1015 11:04:25.362376   40096 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1015 11:04:25.366640   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 11:04:25.418004   40096 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53974 SSHKeyPath:C:\Users\ADMIN\.minikube\machines\minikube\id_rsa Username:docker}
I1015 11:04:25.532083   40096 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1015 11:04:25.540240   40096 fix.go:56] duration metric: took 6.5506113s for fixHost
I1015 11:04:25.540240   40096 start.go:83] releasing machines lock for "minikube", held for 6.5506113s
I1015 11:04:25.545083   40096 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1015 11:04:25.601940   40096 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I1015 11:04:25.608509   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 11:04:25.609043   40096 ssh_runner.go:195] Run: cat /version.json
I1015 11:04:25.613247   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 11:04:25.662403   40096 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53974 SSHKeyPath:C:\Users\ADMIN\.minikube\machines\minikube\id_rsa Username:docker}
I1015 11:04:25.666215   40096 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53974 SSHKeyPath:C:\Users\ADMIN\.minikube\machines\minikube\id_rsa Username:docker}
W1015 11:04:25.768611   40096 start.go:868] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I1015 11:04:25.774545   40096 ssh_runner.go:195] Run: systemctl --version
I1015 11:04:25.795436   40096 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1015 11:04:25.806691   40096 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1015 11:04:25.824962   40096 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1015 11:04:25.826540   40096 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1015 11:04:25.841522   40096 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1015 11:04:25.841522   40096 start.go:495] detecting cgroup driver to use...
I1015 11:04:25.841522   40096 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1015 11:04:25.843608   40096 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1015 11:04:25.877568   40096 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I1015 11:04:25.902237   40096 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1015 11:04:25.920187   40096 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1015 11:04:25.927122   40096 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1015 11:04:25.951936   40096 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1015 11:04:25.977040   40096 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1015 11:04:26.001602   40096 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
W1015 11:04:26.021165   40096 out.go:285] ❗  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W1015 11:04:26.021165   40096 out.go:285] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1015 11:04:26.026517   40096 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1015 11:04:26.049692   40096 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1015 11:04:26.072343   40096 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1015 11:04:26.098017   40096 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1015 11:04:26.116516   40096 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1015 11:04:26.133218   40096 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1015 11:04:26.149422   40096 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1015 11:04:26.255129   40096 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1015 11:04:26.380219   40096 start.go:495] detecting cgroup driver to use...
I1015 11:04:26.380219   40096 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1015 11:04:26.381944   40096 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1015 11:04:26.403330   40096 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1015 11:04:26.421632   40096 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1015 11:04:26.452267   40096 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1015 11:04:26.472139   40096 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1015 11:04:26.491630   40096 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1015 11:04:26.524983   40096 ssh_runner.go:195] Run: which cri-dockerd
I1015 11:04:26.532944   40096 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1015 11:04:26.547456   40096 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I1015 11:04:26.575475   40096 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1015 11:04:26.665771   40096 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1015 11:04:26.769122   40096 docker.go:575] configuring docker to use "cgroupfs" as cgroup driver...
I1015 11:04:26.769122   40096 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1015 11:04:26.797508   40096 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I1015 11:04:26.817518   40096 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1015 11:04:26.915186   40096 ssh_runner.go:195] Run: sudo systemctl restart docker
I1015 11:04:28.679491   40096 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.7643049s)
I1015 11:04:28.681337   40096 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I1015 11:04:28.700860   40096 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1015 11:04:28.720460   40096 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1015 11:04:28.741437   40096 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1015 11:04:28.761401   40096 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1015 11:04:28.862585   40096 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1015 11:04:28.965691   40096 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1015 11:04:29.044005   40096 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1015 11:04:29.097347   40096 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I1015 11:04:29.116737   40096 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1015 11:04:29.215479   40096 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1015 11:04:29.616455   40096 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1015 11:04:29.636112   40096 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1015 11:04:29.643578   40096 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1015 11:04:29.650598   40096 start.go:563] Will wait 60s for crictl version
I1015 11:04:29.658076   40096 ssh_runner.go:195] Run: which crictl
I1015 11:04:29.665383   40096 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1015 11:04:29.820961   40096 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I1015 11:04:29.825281   40096 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1015 11:04:29.959040   40096 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1015 11:04:29.999810   40096 out.go:252] 🐳  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I1015 11:04:30.006126   40096 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1015 11:04:30.225201   40096 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1015 11:04:30.233685   40096 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1015 11:04:30.240352   40096 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1015 11:04:30.264002   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1015 11:04:30.317406   40096 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1015 11:04:30.317406   40096 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1015 11:04:30.321135   40096 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1015 11:04:30.356787   40096 docker.go:691] Got preloaded images: -- stdout --
secure-todo:latest
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1015 11:04:30.356787   40096 docker.go:621] Images already preloaded, skipping extraction
I1015 11:04:30.361678   40096 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1015 11:04:30.388920   40096 docker.go:691] Got preloaded images: -- stdout --
secure-todo:latest
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1015 11:04:30.388920   40096 cache_images.go:85] Images are preloaded, skipping loading
I1015 11:04:30.388920   40096 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I1015 11:04:30.389998   40096 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1015 11:04:30.394781   40096 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1015 11:04:30.659821   40096 cni.go:84] Creating CNI manager for ""
I1015 11:04:30.659821   40096 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1015 11:04:30.659821   40096 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1015 11:04:30.659821   40096 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1015 11:04:30.660366   40096 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1015 11:04:30.661413   40096 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I1015 11:04:30.679025   40096 binaries.go:44] Found k8s binaries, skipping transfer
I1015 11:04:30.680081   40096 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1015 11:04:30.695475   40096 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1015 11:04:30.723220   40096 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1015 11:04:30.750789   40096 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2209 bytes)
I1015 11:04:30.786066   40096 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1015 11:04:30.792691   40096 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1015 11:04:30.813441   40096 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1015 11:04:30.899086   40096 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1015 11:04:30.937688   40096 certs.go:68] Setting up C:\Users\ADMIN\.minikube\profiles\minikube for IP: 192.168.49.2
I1015 11:04:30.937688   40096 certs.go:194] generating shared ca certs ...
I1015 11:04:30.937688   40096 certs.go:226] acquiring lock for ca certs: {Name:mk82925a48d2c86ca5fc4ff57d865fef300d0d79 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1015 11:04:30.955489   40096 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\ADMIN\.minikube\ca.key
I1015 11:04:30.984728   40096 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\ADMIN\.minikube\proxy-client-ca.key
I1015 11:04:30.984728   40096 certs.go:256] generating profile certs ...
I1015 11:04:30.984728   40096 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\ADMIN\.minikube\profiles\minikube\client.key
I1015 11:04:31.016576   40096 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\ADMIN\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I1015 11:04:31.042552   40096 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\ADMIN\.minikube\profiles\minikube\proxy-client.key
I1015 11:04:31.045721   40096 certs.go:484] found cert: C:\Users\ADMIN\.minikube\certs\ca-key.pem (1675 bytes)
I1015 11:04:31.045721   40096 certs.go:484] found cert: C:\Users\ADMIN\.minikube\certs\ca.pem (1074 bytes)
I1015 11:04:31.046247   40096 certs.go:484] found cert: C:\Users\ADMIN\.minikube\certs\cert.pem (1119 bytes)
I1015 11:04:31.046247   40096 certs.go:484] found cert: C:\Users\ADMIN\.minikube\certs\key.pem (1675 bytes)
I1015 11:04:31.052568   40096 ssh_runner.go:362] scp C:\Users\ADMIN\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1015 11:04:31.100195   40096 ssh_runner.go:362] scp C:\Users\ADMIN\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1015 11:04:31.146847   40096 ssh_runner.go:362] scp C:\Users\ADMIN\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1015 11:04:31.192466   40096 ssh_runner.go:362] scp C:\Users\ADMIN\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1015 11:04:31.237392   40096 ssh_runner.go:362] scp C:\Users\ADMIN\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1015 11:04:31.276656   40096 ssh_runner.go:362] scp C:\Users\ADMIN\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1015 11:04:31.378823   40096 ssh_runner.go:362] scp C:\Users\ADMIN\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1015 11:04:31.482877   40096 ssh_runner.go:362] scp C:\Users\ADMIN\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1015 11:04:31.567354   40096 ssh_runner.go:362] scp C:\Users\ADMIN\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1015 11:04:31.610471   40096 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1015 11:04:31.648860   40096 ssh_runner.go:195] Run: openssl version
I1015 11:04:31.671998   40096 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1015 11:04:31.700326   40096 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1015 11:04:31.707919   40096 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Oct 14 10:47 /usr/share/ca-certificates/minikubeCA.pem
I1015 11:04:31.715841   40096 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1015 11:04:31.727205   40096 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1015 11:04:31.751189   40096 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1015 11:04:31.766356   40096 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1015 11:04:31.784273   40096 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1015 11:04:31.805005   40096 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1015 11:04:31.829286   40096 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1015 11:04:31.848622   40096 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1015 11:04:31.865702   40096 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1015 11:04:31.877209   40096 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1015 11:04:31.881801   40096 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1015 11:04:31.919519   40096 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1015 11:04:31.941188   40096 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1015 11:04:31.941749   40096 kubeadm.go:589] restartPrimaryControlPlane start ...
I1015 11:04:31.944970   40096 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1015 11:04:31.971615   40096 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1015 11:04:31.982896   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1015 11:04:32.045767   40096 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:62516"
I1015 11:04:32.046284   40096 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:62516, want: 127.0.0.1:53973
I1015 11:04:32.046826   40096 kubeconfig.go:62] C:\Users\ADMIN\.kube\config needs updating (will repair): [kubeconfig needs server address update]
I1015 11:04:32.051052   40096 lock.go:35] WriteFile acquiring C:\Users\ADMIN\.kube\config: {Name:mk94148c74e327f44c7853d62e41f4026ea98170 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1015 11:04:32.097390   40096 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1015 11:04:32.166964   40096 kubeadm.go:626] The running cluster does not require reconfiguration: 127.0.0.1
I1015 11:04:32.166964   40096 kubeadm.go:593] duration metric: took 225.2149ms to restartPrimaryControlPlane
I1015 11:04:32.166964   40096 kubeadm.go:394] duration metric: took 289.7551ms to StartCluster
I1015 11:04:32.166964   40096 settings.go:142] acquiring lock: {Name:mkefb151e4e71541fc3a1db2ca8426bbb4b3f89e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1015 11:04:32.166964   40096 settings.go:150] Updating kubeconfig:  C:\Users\ADMIN\.kube\config
I1015 11:04:32.168574   40096 lock.go:35] WriteFile acquiring C:\Users\ADMIN\.kube\config: {Name:mk94148c74e327f44c7853d62e41f4026ea98170 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1015 11:04:32.169659   40096 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1015 11:04:32.170214   40096 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1015 11:04:32.170214   40096 out.go:179] 🔎  Verifying Kubernetes components...
I1015 11:04:32.170214   40096 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1015 11:04:32.171844   40096 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1015 11:04:32.171844   40096 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1015 11:04:32.171844   40096 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1015 11:04:32.171844   40096 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W1015 11:04:32.171844   40096 addons.go:247] addon storage-provisioner should already be in state true
I1015 11:04:32.172384   40096 host.go:66] Checking if "minikube" exists ...
I1015 11:04:32.175067   40096 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1015 11:04:32.188253   40096 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1015 11:04:32.190377   40096 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1015 11:04:32.258947   40096 out.go:179]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1015 11:04:32.259602   40096 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1015 11:04:32.259602   40096 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1015 11:04:32.264803   40096 addons.go:238] Setting addon default-storageclass=true in "minikube"
W1015 11:04:32.264803   40096 addons.go:247] addon default-storageclass should already be in state true
I1015 11:04:32.265426   40096 host.go:66] Checking if "minikube" exists ...
I1015 11:04:32.269682   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 11:04:32.286874   40096 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1015 11:04:32.339631   40096 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53974 SSHKeyPath:C:\Users\ADMIN\.minikube\machines\minikube\id_rsa Username:docker}
I1015 11:04:32.349986   40096 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I1015 11:04:32.349986   40096 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1015 11:04:32.354266   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 11:04:32.427195   40096 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53974 SSHKeyPath:C:\Users\ADMIN\.minikube\machines\minikube\id_rsa Username:docker}
I1015 11:04:32.676753   40096 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1015 11:04:32.691637   40096 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1015 11:04:32.692200   40096 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1015 11:04:32.701960   40096 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1015 11:04:32.761070   40096 api_server.go:52] waiting for apiserver process to appear ...
I1015 11:04:32.764359   40096 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1015 11:04:33.387685   40096 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1015 11:04:33.387685   40096 retry.go:31] will retry after 341.852822ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1015 11:04:33.394533   40096 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1015 11:04:33.394533   40096 retry.go:31] will retry after 200.970405ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1015 11:04:33.397186   40096 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1015 11:04:33.474700   40096 api_server.go:72] duration metric: took 1.3050413s to wait for apiserver process to appear ...
I1015 11:04:33.474700   40096 api_server.go:88] waiting for apiserver healthz status ...
I1015 11:04:33.474700   40096 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53973/healthz ...
I1015 11:04:33.477387   40096 api_server.go:269] stopped: https://127.0.0.1:53973/healthz: Get "https://127.0.0.1:53973/healthz": EOF
I1015 11:04:33.597488   40096 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1015 11:04:33.731817   40096 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1015 11:04:33.972107   40096 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1015 11:04:33.972107   40096 retry.go:31] will retry after 425.493772ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1015 11:04:33.974800   40096 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53973/healthz ...
I1015 11:04:33.978097   40096 api_server.go:269] stopped: https://127.0.0.1:53973/healthz: Get "https://127.0.0.1:53973/healthz": EOF
W1015 11:04:34.086578   40096 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1015 11:04:34.086578   40096 retry.go:31] will retry after 365.754612ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1015 11:04:34.400183   40096 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1015 11:04:34.454355   40096 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1015 11:04:34.475236   40096 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53973/healthz ...
I1015 11:04:36.366263   40096 api_server.go:279] https://127.0.0.1:53973/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1015 11:04:36.366263   40096 api_server.go:103] status: https://127.0.0.1:53973/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1015 11:04:36.366263   40096 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53973/healthz ...
I1015 11:04:36.464058   40096 api_server.go:279] https://127.0.0.1:53973/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1015 11:04:36.464058   40096 api_server.go:103] status: https://127.0.0.1:53973/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1015 11:04:36.475786   40096 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53973/healthz ...
I1015 11:04:36.567394   40096 api_server.go:279] https://127.0.0.1:53973/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1015 11:04:36.567394   40096 api_server.go:103] status: https://127.0.0.1:53973/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1015 11:04:36.975132   40096 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53973/healthz ...
I1015 11:04:36.981933   40096 api_server.go:279] https://127.0.0.1:53973/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1015 11:04:36.981933   40096 api_server.go:103] status: https://127.0.0.1:53973/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1015 11:04:37.201509   40096 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.8013264s)
I1015 11:04:37.201509   40096 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (2.7471543s)
I1015 11:04:37.230033   40096 out.go:179] 🌟  Enabled addons: storage-provisioner, default-storageclass
I1015 11:04:37.230578   40096 addons.go:514] duration metric: took 5.060919s for enable addons: enabled=[storage-provisioner default-storageclass]
I1015 11:04:37.474947   40096 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53973/healthz ...
I1015 11:04:37.481017   40096 api_server.go:279] https://127.0.0.1:53973/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1015 11:04:37.481017   40096 api_server.go:103] status: https://127.0.0.1:53973/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1015 11:04:37.975339   40096 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53973/healthz ...
I1015 11:04:37.982604   40096 api_server.go:279] https://127.0.0.1:53973/healthz returned 200:
ok
I1015 11:04:37.985856   40096 api_server.go:141] control plane version: v1.34.0
I1015 11:04:37.985856   40096 api_server.go:131] duration metric: took 4.5111563s to wait for apiserver health ...
I1015 11:04:37.985856   40096 system_pods.go:43] waiting for kube-system pods to appear ...
I1015 11:04:38.007324   40096 system_pods.go:59] 7 kube-system pods found
I1015 11:04:38.007324   40096 system_pods.go:61] "coredns-66bc5c9577-9l5fc" [a7a43bb1-c8ea-4ca8-adf3-8585fefce1f4] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1015 11:04:38.007324   40096 system_pods.go:61] "etcd-minikube" [6c6fff36-14b4-400d-b2f5-44877f1b4d6d] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1015 11:04:38.007324   40096 system_pods.go:61] "kube-apiserver-minikube" [bb4b216b-d578-4abc-9101-03767501f6ad] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1015 11:04:38.007324   40096 system_pods.go:61] "kube-controller-manager-minikube" [d61eb57c-d0fc-41c3-9885-d243ef9297b2] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1015 11:04:38.007324   40096 system_pods.go:61] "kube-proxy-2ctp2" [00cc8658-d9cd-4a39-b92f-c602d1f45263] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1015 11:04:38.007324   40096 system_pods.go:61] "kube-scheduler-minikube" [c7e3571e-1ff7-47fe-aad1-26dd890b55c6] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1015 11:04:38.007324   40096 system_pods.go:61] "storage-provisioner" [90eee241-d11a-4039-b4b3-30cf296a8958] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1015 11:04:38.007324   40096 system_pods.go:74] duration metric: took 21.4678ms to wait for pod list to return data ...
I1015 11:04:38.007324   40096 kubeadm.go:578] duration metric: took 5.8376654s to wait for: map[apiserver:true system_pods:true]
I1015 11:04:38.007324   40096 node_conditions.go:102] verifying NodePressure condition ...
I1015 11:04:38.063065   40096 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1015 11:04:38.063065   40096 node_conditions.go:123] node cpu capacity is 12
I1015 11:04:38.063623   40096 node_conditions.go:105] duration metric: took 56.2988ms to run NodePressure ...
I1015 11:04:38.063623   40096 start.go:241] waiting for startup goroutines ...
I1015 11:04:38.063623   40096 start.go:246] waiting for cluster config update ...
I1015 11:04:38.063623   40096 start.go:255] writing updated cluster config ...
I1015 11:04:38.085052   40096 ssh_runner.go:195] Run: rm -f paused
I1015 11:04:38.207125   40096 start.go:617] kubectl: 1.34.1, cluster: 1.34.0 (minor skew: 0)
I1015 11:04:38.207838   40096 out.go:179] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Oct 15 05:34:26 minikube dockerd[217]: time="2025-10-15T05:34:26.935156686Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=moby
Oct 15 05:34:26 minikube systemd[1]: docker.service: Deactivated successfully.
Oct 15 05:34:26 minikube systemd[1]: Stopped Docker Application Container Engine.
Oct 15 05:34:26 minikube systemd[1]: docker.service: Consumed 1.195s CPU time.
Oct 15 05:34:27 minikube systemd[1]: Starting Docker Application Container Engine...
Oct 15 05:34:27 minikube dockerd[807]: time="2025-10-15T05:34:27.126854856Z" level=info msg="Starting up"
Oct 15 05:34:27 minikube dockerd[807]: time="2025-10-15T05:34:27.128227672Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Oct 15 05:34:27 minikube dockerd[807]: time="2025-10-15T05:34:27.128350943Z" level=info msg="CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory" dir=/etc/cdi
Oct 15 05:34:27 minikube dockerd[807]: time="2025-10-15T05:34:27.128388316Z" level=info msg="CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory" dir=/var/run/cdi
Oct 15 05:34:27 minikube dockerd[807]: time="2025-10-15T05:34:27.144783969Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Oct 15 05:34:27 minikube dockerd[807]: time="2025-10-15T05:34:27.148934335Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Oct 15 05:34:27 minikube dockerd[807]: time="2025-10-15T05:34:27.170660096Z" level=info msg="Loading containers: start."
Oct 15 05:34:28 minikube dockerd[807]: time="2025-10-15T05:34:28.545655151Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count 5278b3f4d99770abf100f979901bd45acbf8357c4a2d61da98c8f915fe061bc4], retrying...."
Oct 15 05:34:28 minikube dockerd[807]: time="2025-10-15T05:34:28.606820907Z" level=warning msg="error locating sandbox id fa4f7bae8954d132fa37520f5355d02e28d767a6aacdd8769bb671693b55e49b: sandbox fa4f7bae8954d132fa37520f5355d02e28d767a6aacdd8769bb671693b55e49b not found"
Oct 15 05:34:28 minikube dockerd[807]: time="2025-10-15T05:34:28.606873659Z" level=warning msg="error locating sandbox id f0473894a647c1f4edd1a5e6da1177b3d68756deeb91c4739498a3e1f2ba64a9: sandbox f0473894a647c1f4edd1a5e6da1177b3d68756deeb91c4739498a3e1f2ba64a9 not found"
Oct 15 05:34:28 minikube dockerd[807]: time="2025-10-15T05:34:28.606891194Z" level=warning msg="error locating sandbox id ab58ab08b0b4e43355ea6e31d6befd7524a7ee79d8ba70acf51fbe7a7d092383: sandbox ab58ab08b0b4e43355ea6e31d6befd7524a7ee79d8ba70acf51fbe7a7d092383 not found"
Oct 15 05:34:28 minikube dockerd[807]: time="2025-10-15T05:34:28.606903327Z" level=warning msg="error locating sandbox id 24ae27001eb653737acf4c0b6afee44c9cafe3d68a08bc1ee14418ce5980d67b: sandbox 24ae27001eb653737acf4c0b6afee44c9cafe3d68a08bc1ee14418ce5980d67b not found"
Oct 15 05:34:28 minikube dockerd[807]: time="2025-10-15T05:34:28.606921022Z" level=warning msg="error locating sandbox id 2fa4c38dcd7b15877eef47f2f6ab0982ddf4228112ac2ce7f9de70e8d5ce73c2: sandbox 2fa4c38dcd7b15877eef47f2f6ab0982ddf4228112ac2ce7f9de70e8d5ce73c2 not found"
Oct 15 05:34:28 minikube dockerd[807]: time="2025-10-15T05:34:28.606932304Z" level=warning msg="error locating sandbox id 14f4f83c52a17ed62ba0899cc784857a9adc9d47630c1b0307e9229fbd9d7acf: sandbox 14f4f83c52a17ed62ba0899cc784857a9adc9d47630c1b0307e9229fbd9d7acf not found"
Oct 15 05:34:28 minikube dockerd[807]: time="2025-10-15T05:34:28.606940911Z" level=warning msg="error locating sandbox id 6b513ee05ab7a1f7314d182ca7937690db6877d287891850ae9a40f4c56f4850: sandbox 6b513ee05ab7a1f7314d182ca7937690db6877d287891850ae9a40f4c56f4850 not found"
Oct 15 05:34:28 minikube dockerd[807]: time="2025-10-15T05:34:28.606950028Z" level=warning msg="error locating sandbox id a3f49fa4dbeb51ebf1a9640884bb4defeddb759882e049a8c3874f08a89f2ad5: sandbox a3f49fa4dbeb51ebf1a9640884bb4defeddb759882e049a8c3874f08a89f2ad5 not found"
Oct 15 05:34:28 minikube dockerd[807]: time="2025-10-15T05:34:28.607400942Z" level=info msg="Loading containers: done."
Oct 15 05:34:28 minikube dockerd[807]: time="2025-10-15T05:34:28.627345835Z" level=info msg="Docker daemon" commit=249d679 containerd-snapshotter=false storage-driver=overlay2 version=28.4.0
Oct 15 05:34:28 minikube dockerd[807]: time="2025-10-15T05:34:28.627465278Z" level=info msg="Initializing buildkit"
Oct 15 05:34:28 minikube dockerd[807]: time="2025-10-15T05:34:28.671468700Z" level=info msg="Completed buildkit initialization"
Oct 15 05:34:28 minikube dockerd[807]: time="2025-10-15T05:34:28.676528856Z" level=info msg="Daemon has completed initialization"
Oct 15 05:34:28 minikube dockerd[807]: time="2025-10-15T05:34:28.676649932Z" level=info msg="API listen on /var/run/docker.sock"
Oct 15 05:34:28 minikube dockerd[807]: time="2025-10-15T05:34:28.676651745Z" level=info msg="API listen on /run/docker.sock"
Oct 15 05:34:28 minikube dockerd[807]: time="2025-10-15T05:34:28.676666775Z" level=info msg="API listen on [::]:2376"
Oct 15 05:34:28 minikube systemd[1]: Started Docker Application Container Engine.
Oct 15 05:34:29 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Oct 15 05:34:29 minikube cri-dockerd[1132]: time="2025-10-15T05:34:29Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Oct 15 05:34:29 minikube cri-dockerd[1132]: time="2025-10-15T05:34:29Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Oct 15 05:34:29 minikube cri-dockerd[1132]: time="2025-10-15T05:34:29Z" level=info msg="Start docker client with request timeout 0s"
Oct 15 05:34:29 minikube cri-dockerd[1132]: time="2025-10-15T05:34:29Z" level=info msg="Hairpin mode is set to hairpin-veth"
Oct 15 05:34:29 minikube cri-dockerd[1132]: time="2025-10-15T05:34:29Z" level=info msg="Loaded network plugin cni"
Oct 15 05:34:29 minikube cri-dockerd[1132]: time="2025-10-15T05:34:29Z" level=info msg="Docker cri networking managed by network plugin cni"
Oct 15 05:34:29 minikube cri-dockerd[1132]: time="2025-10-15T05:34:29Z" level=info msg="Setting cgroupDriver cgroupfs"
Oct 15 05:34:29 minikube cri-dockerd[1132]: time="2025-10-15T05:34:29Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Oct 15 05:34:29 minikube cri-dockerd[1132]: time="2025-10-15T05:34:29Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Oct 15 05:34:29 minikube cri-dockerd[1132]: time="2025-10-15T05:34:29Z" level=info msg="Start cri-dockerd grpc backend"
Oct 15 05:34:29 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Oct 15 05:34:31 minikube cri-dockerd[1132]: time="2025-10-15T05:34:31Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"todo-deployment-bc7fdbdc6-tjmqm_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ea6912511eee22a89574e092b0b05649be9edc6ebe0d3ea818f73a3bcf0092d8\""
Oct 15 05:34:31 minikube cri-dockerd[1132]: time="2025-10-15T05:34:31Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-66bc5c9577-9l5fc_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"3d05e9046b02841323cf00adbc7ebc81705b72e4982cbb801ef33ddc556fcea4\""
Oct 15 05:34:32 minikube cri-dockerd[1132]: time="2025-10-15T05:34:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e29e5090e79b237310f98cfd4c60a8de39f79cde83021fa3e0b5c18d3143f454/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 15 05:34:32 minikube cri-dockerd[1132]: time="2025-10-15T05:34:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6db438599f4c2f86b5bc3f6fea8de369bea8beb72e14d2f0d0e5d8c75da274c4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 15 05:34:32 minikube cri-dockerd[1132]: time="2025-10-15T05:34:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/468b43524fd3d0d2fd2748212010a32d3f9ff05ec1451e33bde89ab84a750b17/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 15 05:34:32 minikube cri-dockerd[1132]: time="2025-10-15T05:34:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d683012c9b25ff35ce992137c63381d9a07aa79e794ad83fa23c48f1bda6779c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 15 05:34:36 minikube cri-dockerd[1132]: time="2025-10-15T05:34:36Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Oct 15 05:34:38 minikube cri-dockerd[1132]: time="2025-10-15T05:34:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7806b3c4c3ef7bf580837e45c1059889d3305a1c9f0d0ab2081167487d4ff166/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 15 05:34:38 minikube cri-dockerd[1132]: time="2025-10-15T05:34:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/373f8f7c79abdf83b2cdc2ecd1eb26373af6c2c0d126dceb77307a8399f707f9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 15 05:34:38 minikube cri-dockerd[1132]: time="2025-10-15T05:34:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5fbe41b9a2017623e46666a66e8662648840027282c2832abba7fbc523fd30f4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 15 05:34:38 minikube cri-dockerd[1132]: time="2025-10-15T05:34:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3d1b4fe6c16bba96537332f0289f2b3f2b995705969376ddcf8795ba6ecddb43/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 15 05:36:23 minikube cri-dockerd[1132]: time="2025-10-15T05:36:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b8195359282bb9bd2be79a8bd26cb2c306c4dd27a77890be110269e3518468cf/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 15 05:36:26 minikube dockerd[807]: time="2025-10-15T05:36:26.804002610Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 15 05:36:26 minikube dockerd[807]: time="2025-10-15T05:36:26.804071194Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 15 05:36:40 minikube dockerd[807]: time="2025-10-15T05:36:40.188444230Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 15 05:36:40 minikube dockerd[807]: time="2025-10-15T05:36:40.188525759Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 15 05:37:08 minikube dockerd[807]: time="2025-10-15T05:37:08.163979211Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 15 05:37:08 minikube dockerd[807]: time="2025-10-15T05:37:08.164044158Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
76a08bfe8f516       6e38f40d628db       2 minutes ago       Running             storage-provisioner       2                   7806b3c4c3ef7       storage-provisioner
f84e1e8e89233       df0860106674d       3 minutes ago       Running             kube-proxy                1                   373f8f7c79abd       kube-proxy-2ctp2
44a84ac5b5b92       192188c0e357b       3 minutes ago       Running             secure-todo               1                   3d1b4fe6c16bb       todo-deployment-bc7fdbdc6-tjmqm
3a2f8cb0c4841       52546a367cc9e       3 minutes ago       Running             coredns                   1                   5fbe41b9a2017       coredns-66bc5c9577-9l5fc
83c366a7c6b8e       a0af72f2ec6d6       3 minutes ago       Running             kube-controller-manager   1                   d683012c9b25f       kube-controller-manager-minikube
1c639241ceaf0       5f1f5298c888d       3 minutes ago       Running             etcd                      1                   468b43524fd3d       etcd-minikube
aa7069788a2cf       90550c43ad2bc       3 minutes ago       Running             kube-apiserver            1                   e29e5090e79b2       kube-apiserver-minikube
a828fac7c58f9       46169d968e920       3 minutes ago       Running             kube-scheduler            1                   6db438599f4c2       kube-scheduler-minikube
d5234741b92c2       192188c0e357b       19 hours ago        Exited              secure-todo               0                   ea6912511eee2       todo-deployment-bc7fdbdc6-tjmqm
5cb8d14b34a32       6e38f40d628db       19 hours ago        Exited              storage-provisioner       1                   abd16580536f4       storage-provisioner
430ca7fdbd46f       52546a367cc9e       19 hours ago        Exited              coredns                   0                   3d05e9046b028       coredns-66bc5c9577-9l5fc
9f6f50d03ab9e       df0860106674d       19 hours ago        Exited              kube-proxy                0                   bffcbbfe27307       kube-proxy-2ctp2
2579c16548ae6       5f1f5298c888d       19 hours ago        Exited              etcd                      0                   9121144fad0da       etcd-minikube
f5c4e8d925fa2       46169d968e920       19 hours ago        Exited              kube-scheduler            0                   12bfd7d5325cd       kube-scheduler-minikube
8ee25cc2448b5       90550c43ad2bc       19 hours ago        Exited              kube-apiserver            0                   c27de4b4ba961       kube-apiserver-minikube
b9f523f209b5a       a0af72f2ec6d6       19 hours ago        Exited              kube-controller-manager   0                   7d93450085a98       kube-controller-manager-minikube


==> coredns [3a2f8cb0c484] <==
maxprocs: Leaving GOMAXPROCS=12: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:45301 - 2565 "HINFO IN 564435267000013704.5950983325063926860. udp 56 false 512" - - 0 6.00311114s
[ERROR] plugin/errors: 2 564435267000013704.5950983325063926860. HINFO: read udp 10.244.0.5:60320->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:59020 - 48088 "HINFO IN 564435267000013704.5950983325063926860. udp 56 false 512" - - 0 6.001678072s
[ERROR] plugin/errors: 2 564435267000013704.5950983325063926860. HINFO: read udp 10.244.0.5:57468->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:46774 - 6501 "HINFO IN 564435267000013704.5950983325063926860. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.042519657s
[INFO] 127.0.0.1:45629 - 31342 "HINFO IN 564435267000013704.5950983325063926860. udp 56 false 512" - - 0 4.001540316s
[ERROR] plugin/errors: 2 564435267000013704.5950983325063926860. HINFO: read udp 10.244.0.5:35933->192.168.65.254:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/kubernetes: Unhandled Error


==> coredns [430ca7fdbd46] <==
maxprocs: Leaving GOMAXPROCS=12: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:34022 - 44994 "HINFO IN 3482238930442316125.2056364997924453945. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.050514893s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_10_14T16_18_06_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 14 Oct 2025 10:48:03 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 15 Oct 2025 05:37:51 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 15 Oct 2025 05:34:36 +0000   Tue, 14 Oct 2025 10:48:00 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 15 Oct 2025 05:34:36 +0000   Tue, 14 Oct 2025 10:48:00 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 15 Oct 2025 05:34:36 +0000   Tue, 14 Oct 2025 10:48:00 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 15 Oct 2025 05:34:36 +0000   Tue, 14 Oct 2025 10:48:03 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7773392Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7773392Ki
  pods:               110
System Info:
  Machine ID:                 6ef01d686a674857acbeeaf4a7b62350
  System UUID:                6ef01d686a674857acbeeaf4a7b62350
  Boot ID:                    38711154-8305-403d-8c35-7c00e126a57c
  Kernel Version:             6.6.87.2-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                     CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                     ------------  ----------  ---------------  -------------  ---
  default                     jobtracker-deployment-8f55664c7-8jzhs    0 (0%)        0 (0%)      0 (0%)           0 (0%)         91s
  default                     todo-deployment-bc7fdbdc6-tjmqm          0 (0%)        0 (0%)      0 (0%)           0 (0%)         18h
  kube-system                 coredns-66bc5c9577-9l5fc                 100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     18h
  kube-system                 etcd-minikube                            100m (0%)     0 (0%)      100Mi (1%)       0 (0%)         18h
  kube-system                 kube-apiserver-minikube                  250m (2%)     0 (0%)      0 (0%)           0 (0%)         18h
  kube-system                 kube-controller-manager-minikube         200m (1%)     0 (0%)      0 (0%)           0 (0%)         18h
  kube-system                 kube-proxy-2ctp2                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         18h
  kube-system                 kube-scheduler-minikube                  100m (0%)     0 (0%)      0 (0%)           0 (0%)         18h
  kube-system                 storage-provisioner                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         18h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (6%)   0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 3m                     kube-proxy       
  Normal  Starting                 3m23s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  3m23s (x8 over 3m23s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    3m23s (x8 over 3m23s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     3m23s (x7 over 3m23s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  3m23s                  kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           3m15s                  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.004998] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001035] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001157] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001167] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.003932] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000998] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001070] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001339] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.176262] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#602 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.023915] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#767 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.010460] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#831 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.011401] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#895 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.011538] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#10 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.010934] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#127 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +3.022696] WSL (247) ERROR: CheckConnection: getaddrinfo() failed: -5
[Oct14 21:10] netlink: 'init': attribute type 4 has an invalid length.
[  +0.080799] virtiofs: Unknown parameter 'negative_dentry_timeout'
[Oct14 21:46] WSL (247) ERROR: CheckConnection: getaddrinfo() failed: -5
[Oct14 22:03] WSL (247) ERROR: CheckConnection: getaddrinfo() failed: -5
[ +31.208559] WSL (247) ERROR: CheckConnection: getaddrinfo() failed: -5
[Oct14 22:49] WSL (247) ERROR: CheckConnection: getaddrinfo() failed: -5
[Oct15 02:16] WSL (247) ERROR: CheckConnection: getaddrinfo() failed: -5
[Oct15 02:41] WSL (247) ERROR: CheckConnection: getaddrinfo() failed: -5
[Oct15 02:43] WSL (247) ERROR: CheckConnection: getaddrinfo() failed: -5
[Oct15 03:39] WSL (247) ERROR: CheckConnection: getaddrinfo() failed: -5
[Oct15 03:46] WSL (247) ERROR: CheckConnection: getaddrinfo() failed: -5
[Oct15 03:54] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +5.033156] WSL (247) ERROR: CheckConnection: getaddrinfo() failed: -3
[  +5.145769] WSL (247) ERROR: CheckConnection: getaddrinfo() failed: -3
[ +11.610815] WSL (247) ERROR: CheckConnection: getaddrinfo() failed: -5
[Oct15 04:02] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.001325] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.000422] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[Oct15 04:06] Exception: 
[  +0.000038] Operation canceled @p9io.cpp:258 (AcceptAsync)

[  +0.002517] WSL (1 - init()) ERROR: InitEntryUtilityVm:2506: Init has exited. Terminating distribution
[  +0.029879] device offline error, dev sdd, sector 0 op 0x1:(WRITE) flags 0x800 phys_seg 0 prio class 2
[  +0.009518] EXT4-fs (sdd): shut down requested (2)
[  +0.000620] Aborting journal on device sdd-8.
[  +0.001339] device offline error, dev sdd, sector 131074 op 0x1:(WRITE) flags 0x9800 phys_seg 1 prio class 2
[  +0.000859] device offline error, dev sdd, sector 131074 op 0x1:(WRITE) flags 0x9800 phys_seg 1 prio class 2
[  +0.000636] Buffer I/O error on dev sdd, logical block 65537, lost sync page write
[  +0.001125] JBD2: I/O error when updating journal superblock for sdd-8.
[  +0.750124] /sbin/ldconfig: 
[  +0.000006] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.306437] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2058: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.020833] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.292630] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.005427] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000900] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001142] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001221] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.005796] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001158] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000980] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000964] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.169184] netlink: 'init': attribute type 4 has an invalid length.
[  +0.196799] virtiofs: Unknown parameter 'negative_dentry_timeout'


==> etcd [1c639241ceaf] <==
{"level":"warn","ts":"2025-10-15T05:34:35.103680Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41266","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.104382Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41264","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.124210Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41290","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.129056Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41310","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.161855Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41340","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.176578Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41344","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.182074Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41358","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.187216Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41386","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.193080Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41388","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.198001Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41402","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.203259Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41436","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.208746Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41452","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.213207Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41490","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.259201Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41500","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.264463Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41516","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.270658Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41542","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.276474Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41550","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.281620Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41574","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.287869Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41588","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.292552Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41616","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.298702Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41630","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.303862Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41646","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.380128Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41676","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.385940Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41694","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.396407Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41714","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.401476Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41732","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.406169Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41744","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.412265Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41758","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.460419Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41772","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.465440Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41782","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.470914Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41794","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.477404Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41808","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.483431Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41830","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.489162Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41836","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.494012Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41860","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.498912Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41864","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.503885Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41872","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.508591Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41888","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.572652Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41898","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.577771Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41928","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.583419Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41950","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.589520Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41964","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.594731Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41980","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.599624Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42002","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.604506Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42012","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.661977Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42038","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.668198Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42058","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.672620Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42066","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.683142Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42092","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.688839Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42102","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.694680Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42120","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.700478Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42124","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.706607Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42130","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.711536Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42144","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.716694Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42172","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.761579Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42186","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.781729Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42198","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.787002Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42212","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.792167Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42222","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-15T05:34:35.853252Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42254","server-name":"","error":"EOF"}


==> etcd [2579c16548ae] <==
{"level":"info","ts":"2025-10-14T18:03:48.859087Z","caller":"traceutil/trace.go:172","msg":"trace[212458794] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:12969; }","duration":"1.116647855s","start":"2025-10-14T18:03:47.736788Z","end":"2025-10-14T18:03:48.853436Z","steps":["trace[212458794] 'agreement among raft nodes before linearized reading'  (duration: 1.113234668s)"],"step_count":1}
{"level":"warn","ts":"2025-10-14T18:03:48.859249Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-14T18:03:47.736764Z","time spent":"1.122446588s","remote":"127.0.0.1:35914","response type":"/etcdserverpb.KV/Range","request count":0,"request size":69,"response count":1,"response size":625,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 "}
{"level":"warn","ts":"2025-10-14T18:03:48.859249Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-14T18:03:47.748036Z","time spent":"1.111175245s","remote":"127.0.0.1:36180","response type":"/etcdserverpb.KV/Range","request count":0,"request size":54,"response count":3,"response size":1780,"request content":"key:\"/registry/ipaddresses/\" range_end:\"/registry/ipaddresses0\" limit:10000 revision:12968 "}
{"level":"warn","ts":"2025-10-14T18:03:48.855516Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"420.329595ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/resourcequotas\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-14T18:03:48.854335Z","caller":"traceutil/trace.go:172","msg":"trace[81207507] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:12969; }","duration":"1.019053483s","start":"2025-10-14T18:03:47.835268Z","end":"2025-10-14T18:03:48.854321Z","steps":["trace[81207507] 'agreement among raft nodes before linearized reading'  (duration: 1.016046316s)"],"step_count":1}
{"level":"info","ts":"2025-10-14T18:03:48.934893Z","caller":"traceutil/trace.go:172","msg":"trace[1453767655] range","detail":"{range_begin:/registry/resourcequotas; range_end:; response_count:0; response_revision:12969; }","duration":"499.703081ms","start":"2025-10-14T18:03:48.435161Z","end":"2025-10-14T18:03:48.934864Z","steps":["trace[1453767655] 'agreement among raft nodes before linearized reading'  (duration: 416.220967ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-14T18:03:48.945060Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-14T18:03:48.435139Z","time spent":"509.883867ms","remote":"127.0.0.1:35752","response type":"/etcdserverpb.KV/Range","request count":0,"request size":28,"response count":0,"response size":29,"request content":"key:\"/registry/resourcequotas\" limit:1 "}
{"level":"info","ts":"2025-10-14T18:06:56.986913Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":12882}
{"level":"info","ts":"2025-10-14T18:06:56.991366Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":12882,"took":"3.204273ms","hash":466022852,"current-db-size-bytes":1519616,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":851968,"current-db-size-in-use":"852 kB"}
{"level":"info","ts":"2025-10-14T18:06:56.991442Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":466022852,"revision":12882,"compact-revision":12643}
{"level":"info","ts":"2025-10-14T18:11:56.984450Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":13122}
{"level":"info","ts":"2025-10-14T18:11:56.988348Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":13122,"took":"3.170686ms","hash":4133165270,"current-db-size-bytes":1519616,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":856064,"current-db-size-in-use":"856 kB"}
{"level":"info","ts":"2025-10-14T18:11:56.988435Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":4133165270,"revision":13122,"compact-revision":12882}
{"level":"info","ts":"2025-10-14T18:16:56.979953Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":13361}
{"level":"info","ts":"2025-10-14T18:16:56.983919Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":13361,"took":"3.426311ms","hash":1897024316,"current-db-size-bytes":1519616,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":860160,"current-db-size-in-use":"860 kB"}
{"level":"info","ts":"2025-10-14T18:16:56.984011Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1897024316,"revision":13361,"compact-revision":13122}
{"level":"info","ts":"2025-10-14T18:21:56.971461Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":13600}
{"level":"info","ts":"2025-10-14T18:21:56.974671Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":13600,"took":"2.926382ms","hash":3535528828,"current-db-size-bytes":1519616,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":860160,"current-db-size-in-use":"860 kB"}
{"level":"info","ts":"2025-10-14T18:21:56.974738Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3535528828,"revision":13600,"compact-revision":13361}
{"level":"info","ts":"2025-10-14T18:26:56.961920Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":13841}
{"level":"info","ts":"2025-10-14T18:26:56.965342Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":13841,"took":"3.096782ms","hash":3153521325,"current-db-size-bytes":1519616,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":860160,"current-db-size-in-use":"860 kB"}
{"level":"info","ts":"2025-10-14T18:26:56.965409Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3153521325,"revision":13841,"compact-revision":13600}
{"level":"info","ts":"2025-10-14T18:38:58.224213Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":14080}
{"level":"info","ts":"2025-10-14T18:38:58.228374Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":14080,"took":"3.868566ms","hash":3502168907,"current-db-size-bytes":1519616,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":872448,"current-db-size-in-use":"872 kB"}
{"level":"info","ts":"2025-10-14T18:38:58.228433Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3502168907,"revision":14080,"compact-revision":13841}
{"level":"info","ts":"2025-10-14T18:43:58.213793Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":14320}
{"level":"info","ts":"2025-10-14T18:43:58.217705Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":14320,"took":"3.608566ms","hash":601969755,"current-db-size-bytes":1519616,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":872448,"current-db-size-in-use":"872 kB"}
{"level":"info","ts":"2025-10-14T18:43:58.217759Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":601969755,"revision":14320,"compact-revision":14080}
{"level":"info","ts":"2025-10-14T18:48:58.205121Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":14561}
{"level":"info","ts":"2025-10-14T18:48:58.208464Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":14561,"took":"2.975042ms","hash":2622735928,"current-db-size-bytes":1519616,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":860160,"current-db-size-in-use":"860 kB"}
{"level":"info","ts":"2025-10-14T18:48:58.208527Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2622735928,"revision":14561,"compact-revision":14320}
{"level":"info","ts":"2025-10-14T18:53:58.198192Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":14800}
{"level":"info","ts":"2025-10-14T18:53:58.201533Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":14800,"took":"2.863203ms","hash":362180923,"current-db-size-bytes":1519616,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":856064,"current-db-size-in-use":"856 kB"}
{"level":"info","ts":"2025-10-14T18:53:58.201602Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":362180923,"revision":14800,"compact-revision":14561}
{"level":"info","ts":"2025-10-14T18:58:58.187653Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":15041}
{"level":"info","ts":"2025-10-14T18:58:58.190782Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":15041,"took":"2.874669ms","hash":2580066264,"current-db-size-bytes":1519616,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":856064,"current-db-size-in-use":"856 kB"}
{"level":"info","ts":"2025-10-14T18:58:58.190839Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2580066264,"revision":15041,"compact-revision":14800}
{"level":"info","ts":"2025-10-14T19:03:58.177787Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":15280}
{"level":"info","ts":"2025-10-14T19:03:58.180982Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":15280,"took":"2.848309ms","hash":814699985,"current-db-size-bytes":1519616,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":856064,"current-db-size-in-use":"856 kB"}
{"level":"info","ts":"2025-10-14T19:03:58.181061Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":814699985,"revision":15280,"compact-revision":15041}
{"level":"info","ts":"2025-10-14T19:08:58.169262Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":15520}
{"level":"info","ts":"2025-10-14T19:08:58.172558Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":15520,"took":"2.995895ms","hash":3705935161,"current-db-size-bytes":1519616,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":860160,"current-db-size-in-use":"860 kB"}
{"level":"info","ts":"2025-10-14T19:08:58.172659Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3705935161,"revision":15520,"compact-revision":15280}
{"level":"info","ts":"2025-10-14T19:13:58.156983Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":15760}
{"level":"info","ts":"2025-10-14T19:13:58.160939Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":15760,"took":"3.252106ms","hash":1816015840,"current-db-size-bytes":1519616,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":864256,"current-db-size-in-use":"864 kB"}
{"level":"info","ts":"2025-10-14T19:13:58.161027Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1816015840,"revision":15760,"compact-revision":15520}
{"level":"info","ts":"2025-10-14T19:14:23.363295Z","caller":"etcdserver/server.go:2185","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":20002,"local-member-snapshot-index":10001,"local-member-snapshot-count":10000,"snapshot-forced":false}
{"level":"info","ts":"2025-10-14T19:14:23.369806Z","caller":"etcdserver/server.go:2230","msg":"saved snapshot to disk","snapshot-index":20002}
{"level":"info","ts":"2025-10-14T19:18:58.145444Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":16000}
{"level":"info","ts":"2025-10-14T19:18:58.148367Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":16000,"took":"2.651812ms","hash":378392335,"current-db-size-bytes":1519616,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":864256,"current-db-size-in-use":"864 kB"}
{"level":"info","ts":"2025-10-14T19:18:58.148401Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":378392335,"revision":16000,"compact-revision":15760}
{"level":"info","ts":"2025-10-14T19:23:58.137561Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":16240}
{"level":"info","ts":"2025-10-14T19:23:58.141930Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":16240,"took":"3.938438ms","hash":358162291,"current-db-size-bytes":1519616,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":860160,"current-db-size-in-use":"860 kB"}
{"level":"info","ts":"2025-10-14T19:23:58.141992Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":358162291,"revision":16240,"compact-revision":16000}
{"level":"info","ts":"2025-10-14T19:28:58.128642Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":16479}
{"level":"info","ts":"2025-10-14T19:28:58.132227Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":16479,"took":"3.243289ms","hash":3095829173,"current-db-size-bytes":1519616,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":856064,"current-db-size-in-use":"856 kB"}
{"level":"info","ts":"2025-10-14T19:28:58.132305Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3095829173,"revision":16479,"compact-revision":16240}
{"level":"info","ts":"2025-10-14T19:34:22.068182Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":16719}
{"level":"info","ts":"2025-10-14T19:34:22.071880Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":16719,"took":"3.37124ms","hash":1741946546,"current-db-size-bytes":1519616,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":856064,"current-db-size-in-use":"856 kB"}
{"level":"info","ts":"2025-10-14T19:34:22.071956Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1741946546,"revision":16719,"compact-revision":16479}


==> kernel <==
 05:37:54 up  8:28,  0 users,  load average: 0.23, 0.21, 0.11
Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [8ee25cc2448b] <==
I1014 19:01:30.352179       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:02:33.374016       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:02:44.634454       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:03:35.111638       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:03:47.094024       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:04:52.601587       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:05:03.748352       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:06:14.275782       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:06:20.424367       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:07:28.999701       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:07:34.253533       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:08:39.542588       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:08:41.919747       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:09:00.139699       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1014 19:09:50.766745       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:09:52.586672       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:10:53.751078       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:10:54.481826       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:11:57.004119       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:11:59.767283       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:13:18.829476       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:13:23.250852       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:14:31.512407       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:14:35.396580       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:15:44.895425       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:15:52.792175       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:16:55.890725       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:16:56.234120       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:18:02.310795       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:18:06.070851       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:19:00.101442       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1014 19:19:25.528626       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:19:31.712491       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:20:53.533775       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:21:00.391810       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:21:57.683459       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:22:29.459815       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:23:07.763945       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:23:36.229314       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:24:34.513018       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:24:54.071291       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:25:55.991909       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:25:59.550265       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:27:00.079611       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:27:18.759895       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:28:23.095035       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:28:29.875663       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:29:00.062082       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1014 19:29:31.632822       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:29:34.965725       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:30:49.835928       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:30:51.523200       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:32:09.745527       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:32:20.340091       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:33:19.950394       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:34:13.314899       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:35:00.116958       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:35:16.570117       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:36:02.066478       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1014 19:36:43.491860       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-apiserver [aa7069788a2c] <==
I1015 05:34:36.308363       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1015 05:34:36.308405       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1015 05:34:36.308713       1 controller.go:119] Starting legacy_token_tracking_controller
I1015 05:34:36.308754       1 shared_informer.go:349] "Waiting for caches to sync" controller="configmaps"
I1015 05:34:36.308866       1 customresource_discovery_controller.go:294] Starting DiscoveryController
I1015 05:34:36.308907       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1015 05:34:36.309135       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
I1015 05:34:36.309184       1 shared_informer.go:349] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I1015 05:34:36.309253       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1015 05:34:36.309314       1 controller.go:142] Starting OpenAPI controller
I1015 05:34:36.309365       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1015 05:34:36.309389       1 controller.go:90] Starting OpenAPI V3 controller
I1015 05:34:36.309434       1 naming_controller.go:299] Starting NamingConditionController
I1015 05:34:36.309454       1 establishing_controller.go:81] Starting EstablishingController
I1015 05:34:36.309629       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1015 05:34:36.309658       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1015 05:34:36.309673       1 crd_finalizer.go:269] Starting CRDFinalizer
I1015 05:34:36.309318       1 default_servicecidr_controller.go:111] Starting kubernetes-service-cidr-controller
I1015 05:34:36.309695       1 shared_informer.go:349] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I1015 05:34:36.309389       1 repairip.go:210] Starting ipallocator-repair-controller
I1015 05:34:36.309714       1 shared_informer.go:349] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I1015 05:34:36.310976       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1015 05:34:36.312389       1 aggregator.go:169] waiting for initial CRD sync...
I1015 05:34:36.358773       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1015 05:34:36.358798       1 shared_informer.go:349] "Waiting for caches to sync" controller="crd-autoregister"
I1015 05:34:36.457808       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
I1015 05:34:36.458847       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I1015 05:34:36.458956       1 aggregator.go:171] initial CRD sync complete...
I1015 05:34:36.459035       1 autoregister_controller.go:144] Starting autoregister controller
I1015 05:34:36.459050       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1015 05:34:36.476398       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I1015 05:34:36.557927       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
I1015 05:34:36.558032       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I1015 05:34:36.558109       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I1015 05:34:36.558159       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I1015 05:34:36.558114       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I1015 05:34:36.558268       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1015 05:34:36.558264       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1015 05:34:36.558288       1 cache.go:39] Caches are synced for LocalAvailability controller
I1015 05:34:36.558321       1 policy_source.go:240] refreshing policies
I1015 05:34:36.558328       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1015 05:34:36.558398       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1015 05:34:36.558473       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1015 05:34:36.559286       1 cache.go:39] Caches are synced for autoregister controller
I1015 05:34:36.564544       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I1015 05:34:36.567305       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
E1015 05:34:36.569827       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
E1015 05:34:36.570067       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: a26f3bb5-fc48-4d94-b100-a2965e003452, UID in object meta: "
I1015 05:34:37.063840       1 controller.go:667] quota admission added evaluator for: endpoints
I1015 05:34:37.315255       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1015 05:34:37.377119       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I1015 05:34:40.181684       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I1015 05:34:40.230335       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I1015 05:34:40.279181       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1015 05:34:40.380113       1 controller.go:667] quota admission added evaluator for: deployments.apps
I1015 05:36:00.231492       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1015 05:36:04.379357       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1015 05:36:23.363835       1 alloc.go:328] "allocated clusterIPs" service="default/jobtracker-service" clusterIPs={"IPv4":"10.103.120.238"}
I1015 05:37:24.324649       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1015 05:37:28.568254       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-controller-manager [83c366a7c6b8] <==
I1015 05:34:39.727677       1 gc_controller.go:99] "Starting GC controller" logger="pod-garbage-collector-controller"
I1015 05:34:39.727773       1 shared_informer.go:349] "Waiting for caches to sync" controller="GC"
I1015 05:34:39.776600       1 controllermanager.go:781] "Started controller" controller="service-cidr-controller"
I1015 05:34:39.776799       1 servicecidrs_controller.go:137] "Starting" logger="service-cidr-controller" controller="service-cidr-controller"
I1015 05:34:39.776835       1 shared_informer.go:349] "Waiting for caches to sync" controller="service-cidr-controller"
I1015 05:34:39.785381       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1015 05:34:39.791186       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1015 05:34:39.792895       1 shared_informer.go:356] "Caches are synced" controller="job"
I1015 05:34:39.793506       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1015 05:34:39.794715       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1015 05:34:39.795485       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1015 05:34:39.795766       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1015 05:34:39.797073       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1015 05:34:39.798405       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1015 05:34:39.800082       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1015 05:34:39.801041       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1015 05:34:39.805179       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1015 05:34:39.857716       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1015 05:34:39.857758       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1015 05:34:39.857796       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1015 05:34:39.857908       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1015 05:34:39.857932       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1015 05:34:39.857989       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1015 05:34:39.857991       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1015 05:34:39.858025       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1015 05:34:39.858183       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1015 05:34:39.858183       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1015 05:34:39.858687       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1015 05:34:39.859001       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1015 05:34:39.859040       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1015 05:34:39.862561       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1015 05:34:39.864892       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1015 05:34:39.867131       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1015 05:34:39.870536       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1015 05:34:39.877152       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1015 05:34:39.877200       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1015 05:34:39.877278       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1015 05:34:39.877325       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1015 05:34:39.877371       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1015 05:34:39.877371       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1015 05:34:39.877692       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1015 05:34:39.877703       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1015 05:34:39.880870       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1015 05:34:39.880919       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1015 05:34:39.880930       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I1015 05:34:39.882641       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1015 05:34:39.883885       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1015 05:34:39.885480       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1015 05:34:39.886647       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1015 05:34:39.887971       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1015 05:34:39.889151       1 shared_informer.go:356] "Caches are synced" controller="node"
I1015 05:34:39.889219       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1015 05:34:39.889241       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1015 05:34:39.889246       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1015 05:34:39.889251       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1015 05:34:39.889452       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1015 05:34:39.890365       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1015 05:34:39.890896       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1015 05:34:39.893936       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1015 05:34:39.896453       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"


==> kube-controller-manager [b9f523f209b5] <==
I1014 10:48:10.318417       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1014 10:48:10.329943       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1014 10:48:10.330178       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1014 10:48:10.330375       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1014 10:48:10.330575       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1014 10:48:10.330693       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1014 10:48:10.330963       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1014 10:48:10.331103       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1014 10:48:10.331119       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1014 10:48:10.331104       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1014 10:48:10.337987       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1014 10:48:10.338048       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1014 10:48:10.338061       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1014 10:48:10.338101       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1014 10:48:10.339120       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1014 10:48:10.341061       1 shared_informer.go:356] "Caches are synced" controller="node"
I1014 10:48:10.341180       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1014 10:48:10.341236       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1014 10:48:10.341247       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1014 10:48:10.341256       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1014 10:48:10.348222       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1014 10:48:10.361732       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1014 10:48:10.417353       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1014 10:48:10.417783       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1014 10:48:10.418403       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1014 10:48:10.419259       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1014 10:48:10.419293       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1014 10:48:10.419307       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1014 10:48:10.419320       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1014 10:48:10.419325       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1014 10:48:10.419647       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1014 10:48:10.419675       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1014 10:48:10.419720       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1014 10:48:10.420669       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1014 10:48:10.421049       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1014 10:48:10.421884       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1014 10:48:10.421930       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1014 10:48:10.421947       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I1014 10:48:10.422617       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1014 10:48:10.423893       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1014 10:48:10.423999       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1014 10:48:10.424097       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1014 10:48:10.424156       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1014 10:48:10.425354       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1014 10:48:10.425523       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1014 10:48:10.427055       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1014 10:48:10.428629       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1014 10:48:10.428741       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1014 10:48:10.430092       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1014 10:48:10.430157       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1014 10:48:10.430316       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1014 10:48:10.430434       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1014 10:48:10.431031       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1014 10:48:10.431968       1 shared_informer.go:356] "Caches are synced" controller="job"
I1014 10:48:10.438122       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
E1014 12:51:01.444366       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1014 12:51:01.517910       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I1014 13:42:39.385172       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." logger="certificatesigningrequest-cleaner-controller" csr="csr-n8d84" approvedExpiration="1h0m0s"
E1014 15:03:57.722030       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1014 15:03:57.775209       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"


==> kube-proxy [9f6f50d03ab9] <==
I1014 10:48:12.450221       1 server_linux.go:53] "Using iptables proxy"
I1014 10:48:12.658009       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1014 10:48:12.758625       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1014 10:48:12.758751       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1014 10:48:12.758899       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1014 10:48:12.850392       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1014 10:48:12.850502       1 server_linux.go:132] "Using iptables Proxier"
I1014 10:48:12.861821       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1014 10:48:12.868591       1 server.go:527] "Version info" version="v1.34.0"
I1014 10:48:12.868651       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1014 10:48:12.870417       1 config.go:106] "Starting endpoint slice config controller"
I1014 10:48:12.870459       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1014 10:48:12.870468       1 config.go:403] "Starting serviceCIDR config controller"
I1014 10:48:12.870476       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1014 10:48:12.870512       1 config.go:200] "Starting service config controller"
I1014 10:48:12.870521       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1014 10:48:12.870563       1 config.go:309] "Starting node config controller"
I1014 10:48:12.870584       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1014 10:48:12.970970       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1014 10:48:12.971027       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1014 10:48:12.971053       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1014 10:48:12.971053       1 shared_informer.go:356] "Caches are synced" controller="service config"


==> kube-proxy [f84e1e8e8923] <==
I1015 05:34:52.806256       1 server_linux.go:53] "Using iptables proxy"
I1015 05:34:52.975227       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1015 05:34:53.076173       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1015 05:34:53.076225       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1015 05:34:53.076320       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1015 05:34:53.116615       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1015 05:34:53.116685       1 server_linux.go:132] "Using iptables Proxier"
I1015 05:34:53.125198       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1015 05:34:53.126700       1 server.go:527] "Version info" version="v1.34.0"
I1015 05:34:53.126768       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1015 05:34:53.128555       1 config.go:309] "Starting node config controller"
I1015 05:34:53.128614       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1015 05:34:53.128787       1 config.go:403] "Starting serviceCIDR config controller"
I1015 05:34:53.128823       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1015 05:34:53.128868       1 config.go:200] "Starting service config controller"
I1015 05:34:53.128889       1 config.go:106] "Starting endpoint slice config controller"
I1015 05:34:53.128896       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1015 05:34:53.128896       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1015 05:34:53.229132       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1015 05:34:53.229151       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1015 05:34:53.229240       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1015 05:34:53.229295       1 shared_informer.go:356] "Caches are synced" controller="node config"


==> kube-scheduler [a828fac7c58f] <==
I1015 05:34:34.396016       1 serving.go:386] Generated self-signed cert in-memory
W1015 05:34:36.466890       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1015 05:34:36.466976       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1015 05:34:36.466999       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1015 05:34:36.467012       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1015 05:34:36.566855       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1015 05:34:36.566930       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1015 05:34:36.570304       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1015 05:34:36.570468       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1015 05:34:36.570875       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1015 05:34:36.571934       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1015 05:34:36.671737       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kube-scheduler [f5c4e8d925fa] <==
I1014 10:48:00.865905       1 serving.go:386] Generated self-signed cert in-memory
W1014 10:48:03.318806       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1014 10:48:03.319000       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1014 10:48:03.319025       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1014 10:48:03.319037       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1014 10:48:03.436901       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1014 10:48:03.437027       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1014 10:48:03.440570       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1014 10:48:03.440681       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1014 10:48:03.441840       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1014 10:48:03.442260       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E1014 10:48:03.443629       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1014 10:48:03.443753       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1014 10:48:03.444261       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1014 10:48:03.444388       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1014 10:48:03.445359       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1014 10:48:03.446332       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1014 10:48:03.447198       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1014 10:48:03.524064       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1014 10:48:03.524254       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1014 10:48:03.524655       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1014 10:48:03.524767       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1014 10:48:03.524896       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1014 10:48:03.524830       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1014 10:48:03.524897       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1014 10:48:03.524952       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1014 10:48:03.526055       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1014 10:48:03.526735       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1014 10:48:03.527311       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1014 10:48:03.527160       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1014 10:48:04.335755       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1014 10:48:04.345756       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1014 10:48:04.360291       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1014 10:48:04.369668       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1014 10:48:04.560460       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1014 10:48:04.561319       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1014 10:48:04.570962       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1014 10:48:04.570971       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1014 10:48:04.587411       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1014 10:48:04.610909       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1014 10:48:04.633583       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1014 10:48:04.671612       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1014 10:48:04.672852       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1014 10:48:04.748149       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
I1014 10:48:07.241465       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Oct 15 05:34:36 minikube kubelet[1364]: I1015 05:34:36.494621    1364 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Oct 15 05:34:36 minikube kubelet[1364]: E1015 05:34:36.576355    1364 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Oct 15 05:34:36 minikube kubelet[1364]: I1015 05:34:36.576447    1364 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Oct 15 05:34:36 minikube kubelet[1364]: E1015 05:34:36.666015    1364 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Oct 15 05:34:36 minikube kubelet[1364]: I1015 05:34:36.666052    1364 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Oct 15 05:34:36 minikube kubelet[1364]: I1015 05:34:36.667173    1364 kubelet_node_status.go:124] "Node was previously registered" node="minikube"
Oct 15 05:34:36 minikube kubelet[1364]: I1015 05:34:36.667308    1364 kubelet_node_status.go:78] "Successfully registered node" node="minikube"
Oct 15 05:34:36 minikube kubelet[1364]: I1015 05:34:36.667342    1364 kuberuntime_manager.go:1828] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Oct 15 05:34:36 minikube kubelet[1364]: I1015 05:34:36.668436    1364 kubelet_network.go:47] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Oct 15 05:34:36 minikube kubelet[1364]: E1015 05:34:36.674278    1364 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Oct 15 05:34:36 minikube kubelet[1364]: I1015 05:34:36.674348    1364 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Oct 15 05:34:36 minikube kubelet[1364]: E1015 05:34:36.682988    1364 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Oct 15 05:34:37 minikube kubelet[1364]: I1015 05:34:37.284176    1364 apiserver.go:52] "Watching apiserver"
Oct 15 05:34:37 minikube kubelet[1364]: I1015 05:34:37.297226    1364 desired_state_of_world_populator.go:154] "Finished populating initial desired state of world"
Oct 15 05:34:37 minikube kubelet[1364]: I1015 05:34:37.373637    1364 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/00cc8658-d9cd-4a39-b92f-c602d1f45263-xtables-lock\") pod \"kube-proxy-2ctp2\" (UID: \"00cc8658-d9cd-4a39-b92f-c602d1f45263\") " pod="kube-system/kube-proxy-2ctp2"
Oct 15 05:34:37 minikube kubelet[1364]: I1015 05:34:37.373760    1364 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/00cc8658-d9cd-4a39-b92f-c602d1f45263-lib-modules\") pod \"kube-proxy-2ctp2\" (UID: \"00cc8658-d9cd-4a39-b92f-c602d1f45263\") " pod="kube-system/kube-proxy-2ctp2"
Oct 15 05:34:37 minikube kubelet[1364]: I1015 05:34:37.373859    1364 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/90eee241-d11a-4039-b4b3-30cf296a8958-tmp\") pod \"storage-provisioner\" (UID: \"90eee241-d11a-4039-b4b3-30cf296a8958\") " pod="kube-system/storage-provisioner"
Oct 15 05:34:38 minikube kubelet[1364]: E1015 05:34:38.164341    1364 kuberuntime_manager.go:1449] "Unhandled Error" err="container storage-provisioner start failed in pod storage-provisioner_kube-system(90eee241-d11a-4039-b4b3-30cf296a8958): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars" logger="UnhandledError"
Oct 15 05:34:38 minikube kubelet[1364]: E1015 05:34:38.164442    1364 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/storage-provisioner" podUID="90eee241-d11a-4039-b4b3-30cf296a8958"
Oct 15 05:34:38 minikube kubelet[1364]: E1015 05:34:38.177957    1364 kuberuntime_manager.go:1449] "Unhandled Error" err="container kube-proxy start failed in pod kube-proxy-2ctp2_kube-system(00cc8658-d9cd-4a39-b92f-c602d1f45263): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars" logger="UnhandledError"
Oct 15 05:34:38 minikube kubelet[1364]: E1015 05:34:38.178037    1364 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-proxy\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/kube-proxy-2ctp2" podUID="00cc8658-d9cd-4a39-b92f-c602d1f45263"
Oct 15 05:34:38 minikube kubelet[1364]: I1015 05:34:38.278873    1364 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3d1b4fe6c16bba96537332f0289f2b3f2b995705969376ddcf8795ba6ecddb43"
Oct 15 05:34:38 minikube kubelet[1364]: I1015 05:34:38.290510    1364 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="5fbe41b9a2017623e46666a66e8662648840027282c2832abba7fbc523fd30f4"
Oct 15 05:34:38 minikube kubelet[1364]: I1015 05:34:38.360473    1364 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="7806b3c4c3ef7bf580837e45c1059889d3305a1c9f0d0ab2081167487d4ff166"
Oct 15 05:34:38 minikube kubelet[1364]: I1015 05:34:38.360971    1364 scope.go:117] "RemoveContainer" containerID="5cb8d14b34a325d6165733cf6f14969f7f5679ad4a1d2b8b30dd15cca05139f4"
Oct 15 05:34:38 minikube kubelet[1364]: E1015 05:34:38.362825    1364 kuberuntime_manager.go:1449] "Unhandled Error" err="container storage-provisioner start failed in pod storage-provisioner_kube-system(90eee241-d11a-4039-b4b3-30cf296a8958): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars" logger="UnhandledError"
Oct 15 05:34:38 minikube kubelet[1364]: E1015 05:34:38.362898    1364 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/storage-provisioner" podUID="90eee241-d11a-4039-b4b3-30cf296a8958"
Oct 15 05:34:38 minikube kubelet[1364]: I1015 05:34:38.371834    1364 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="373f8f7c79abdf83b2cdc2ecd1eb26373af6c2c0d126dceb77307a8399f707f9"
Oct 15 05:34:38 minikube kubelet[1364]: I1015 05:34:38.372252    1364 scope.go:117] "RemoveContainer" containerID="9f6f50d03ab9ecba8cd7f8a610e72f345ac8eabe6ccaba56da28518708ee425c"
Oct 15 05:34:38 minikube kubelet[1364]: E1015 05:34:38.374646    1364 kuberuntime_manager.go:1449] "Unhandled Error" err="container kube-proxy start failed in pod kube-proxy-2ctp2_kube-system(00cc8658-d9cd-4a39-b92f-c602d1f45263): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars" logger="UnhandledError"
Oct 15 05:34:38 minikube kubelet[1364]: E1015 05:34:38.374705    1364 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-proxy\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/kube-proxy-2ctp2" podUID="00cc8658-d9cd-4a39-b92f-c602d1f45263"
Oct 15 05:34:39 minikube kubelet[1364]: I1015 05:34:39.424788    1364 scope.go:117] "RemoveContainer" containerID="5cb8d14b34a325d6165733cf6f14969f7f5679ad4a1d2b8b30dd15cca05139f4"
Oct 15 05:34:39 minikube kubelet[1364]: E1015 05:34:39.424971    1364 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(90eee241-d11a-4039-b4b3-30cf296a8958)\"" pod="kube-system/storage-provisioner" podUID="90eee241-d11a-4039-b4b3-30cf296a8958"
Oct 15 05:34:39 minikube kubelet[1364]: I1015 05:34:39.467966    1364 scope.go:117] "RemoveContainer" containerID="9f6f50d03ab9ecba8cd7f8a610e72f345ac8eabe6ccaba56da28518708ee425c"
Oct 15 05:34:39 minikube kubelet[1364]: E1015 05:34:39.468116    1364 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-proxy\" with CrashLoopBackOff: \"back-off 20s restarting failed container=kube-proxy pod=kube-proxy-2ctp2_kube-system(00cc8658-d9cd-4a39-b92f-c602d1f45263)\"" pod="kube-system/kube-proxy-2ctp2" podUID="00cc8658-d9cd-4a39-b92f-c602d1f45263"
Oct 15 05:34:40 minikube kubelet[1364]: I1015 05:34:40.474031    1364 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Oct 15 05:34:46 minikube kubelet[1364]: I1015 05:34:46.157825    1364 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Oct 15 05:34:52 minikube kubelet[1364]: I1015 05:34:52.383731    1364 scope.go:117] "RemoveContainer" containerID="9f6f50d03ab9ecba8cd7f8a610e72f345ac8eabe6ccaba56da28518708ee425c"
Oct 15 05:34:54 minikube kubelet[1364]: I1015 05:34:54.383019    1364 scope.go:117] "RemoveContainer" containerID="5cb8d14b34a325d6165733cf6f14969f7f5679ad4a1d2b8b30dd15cca05139f4"
Oct 15 05:36:23 minikube kubelet[1364]: I1015 05:36:23.453649    1364 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-2f2hh\" (UniqueName: \"kubernetes.io/projected/18ad6713-92ef-4d99-9080-02a28bf108eb-kube-api-access-2f2hh\") pod \"jobtracker-deployment-8f55664c7-8jzhs\" (UID: \"18ad6713-92ef-4d99-9080-02a28bf108eb\") " pod="default/jobtracker-deployment-8f55664c7-8jzhs"
Oct 15 05:36:26 minikube kubelet[1364]: E1015 05:36:26.806868    1364 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for jobtracker, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="jobtracker:lab"
Oct 15 05:36:26 minikube kubelet[1364]: E1015 05:36:26.806994    1364 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: pull access denied for jobtracker, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="jobtracker:lab"
Oct 15 05:36:26 minikube kubelet[1364]: E1015 05:36:26.807153    1364 kuberuntime_manager.go:1449] "Unhandled Error" err="container jobtracker start failed in pod jobtracker-deployment-8f55664c7-8jzhs_default(18ad6713-92ef-4d99-9080-02a28bf108eb): ErrImagePull: Error response from daemon: pull access denied for jobtracker, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Oct 15 05:36:26 minikube kubelet[1364]: E1015 05:36:26.807194    1364 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jobtracker\" with ErrImagePull: \"Error response from daemon: pull access denied for jobtracker, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/jobtracker-deployment-8f55664c7-8jzhs" podUID="18ad6713-92ef-4d99-9080-02a28bf108eb"
Oct 15 05:36:27 minikube kubelet[1364]: E1015 05:36:27.277192    1364 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jobtracker\" with ImagePullBackOff: \"Back-off pulling image \\\"jobtracker:lab\\\": ErrImagePull: Error response from daemon: pull access denied for jobtracker, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/jobtracker-deployment-8f55664c7-8jzhs" podUID="18ad6713-92ef-4d99-9080-02a28bf108eb"
Oct 15 05:36:40 minikube kubelet[1364]: E1015 05:36:40.191721    1364 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for jobtracker, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="jobtracker:lab"
Oct 15 05:36:40 minikube kubelet[1364]: E1015 05:36:40.191798    1364 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: pull access denied for jobtracker, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="jobtracker:lab"
Oct 15 05:36:40 minikube kubelet[1364]: E1015 05:36:40.191901    1364 kuberuntime_manager.go:1449] "Unhandled Error" err="container jobtracker start failed in pod jobtracker-deployment-8f55664c7-8jzhs_default(18ad6713-92ef-4d99-9080-02a28bf108eb): ErrImagePull: Error response from daemon: pull access denied for jobtracker, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Oct 15 05:36:40 minikube kubelet[1364]: E1015 05:36:40.191940    1364 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jobtracker\" with ErrImagePull: \"Error response from daemon: pull access denied for jobtracker, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/jobtracker-deployment-8f55664c7-8jzhs" podUID="18ad6713-92ef-4d99-9080-02a28bf108eb"
Oct 15 05:36:53 minikube kubelet[1364]: E1015 05:36:53.379769    1364 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jobtracker\" with ImagePullBackOff: \"Back-off pulling image \\\"jobtracker:lab\\\": ErrImagePull: Error response from daemon: pull access denied for jobtracker, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/jobtracker-deployment-8f55664c7-8jzhs" podUID="18ad6713-92ef-4d99-9080-02a28bf108eb"
Oct 15 05:37:08 minikube kubelet[1364]: E1015 05:37:08.167146    1364 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for jobtracker, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="jobtracker:lab"
Oct 15 05:37:08 minikube kubelet[1364]: E1015 05:37:08.167219    1364 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: pull access denied for jobtracker, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="jobtracker:lab"
Oct 15 05:37:08 minikube kubelet[1364]: E1015 05:37:08.167352    1364 kuberuntime_manager.go:1449] "Unhandled Error" err="container jobtracker start failed in pod jobtracker-deployment-8f55664c7-8jzhs_default(18ad6713-92ef-4d99-9080-02a28bf108eb): ErrImagePull: Error response from daemon: pull access denied for jobtracker, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Oct 15 05:37:08 minikube kubelet[1364]: E1015 05:37:08.167386    1364 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jobtracker\" with ErrImagePull: \"Error response from daemon: pull access denied for jobtracker, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/jobtracker-deployment-8f55664c7-8jzhs" podUID="18ad6713-92ef-4d99-9080-02a28bf108eb"
Oct 15 05:37:22 minikube kubelet[1364]: E1015 05:37:22.378870    1364 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jobtracker\" with ImagePullBackOff: \"Back-off pulling image \\\"jobtracker:lab\\\": ErrImagePull: Error response from daemon: pull access denied for jobtracker, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/jobtracker-deployment-8f55664c7-8jzhs" podUID="18ad6713-92ef-4d99-9080-02a28bf108eb"
Oct 15 05:37:36 minikube kubelet[1364]: E1015 05:37:36.377779    1364 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jobtracker\" with ImagePullBackOff: \"Back-off pulling image \\\"jobtracker:lab\\\": ErrImagePull: Error response from daemon: pull access denied for jobtracker, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/jobtracker-deployment-8f55664c7-8jzhs" podUID="18ad6713-92ef-4d99-9080-02a28bf108eb"
Oct 15 05:37:54 minikube kubelet[1364]: E1015 05:37:54.138904    1364 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for jobtracker, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="jobtracker:lab"
Oct 15 05:37:54 minikube kubelet[1364]: E1015 05:37:54.138975    1364 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: pull access denied for jobtracker, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="jobtracker:lab"
Oct 15 05:37:54 minikube kubelet[1364]: E1015 05:37:54.139096    1364 kuberuntime_manager.go:1449] "Unhandled Error" err="container jobtracker start failed in pod jobtracker-deployment-8f55664c7-8jzhs_default(18ad6713-92ef-4d99-9080-02a28bf108eb): ErrImagePull: Error response from daemon: pull access denied for jobtracker, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Oct 15 05:37:54 minikube kubelet[1364]: E1015 05:37:54.139129    1364 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jobtracker\" with ErrImagePull: \"Error response from daemon: pull access denied for jobtracker, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/jobtracker-deployment-8f55664c7-8jzhs" podUID="18ad6713-92ef-4d99-9080-02a28bf108eb"


==> storage-provisioner [5cb8d14b34a3] <==
W1014 19:35:54.875368       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:35:54.884465       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:35:56.891677       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:35:56.901771       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:35:58.906889       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:35:58.913558       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:00.917854       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:00.924515       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:02.928616       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:02.936952       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:04.938798       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:04.945370       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:06.949634       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:06.956062       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:08.959802       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:08.965741       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:10.969847       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:10.978255       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:13.005179       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:13.013734       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:15.017233       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:15.025196       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:17.030103       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:17.036781       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:19.040800       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:19.045594       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:21.049410       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:21.056119       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:23.061005       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:23.068409       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:25.072602       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:25.079704       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:27.084476       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:27.091248       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:29.095224       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:29.105810       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:31.111310       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:31.119574       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:33.124300       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:33.132564       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:35.133699       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:35.139068       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:37.144144       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:37.152257       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:39.157245       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:39.165292       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:41.169545       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:41.177267       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:43.183060       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:43.190498       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:45.194913       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:45.201499       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:47.206320       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:47.226639       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:49.231091       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:49.236995       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:51.241095       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:51.248645       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:53.253214       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1014 19:36:53.329490       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [76a08bfe8f51] <==
W1015 05:36:54.575282       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:36:54.581354       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:36:56.585451       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:36:56.591904       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:36:58.594408       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:36:58.599270       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:00.601926       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:00.606431       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:02.610263       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:02.614736       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:04.618708       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:04.626179       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:06.630443       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:06.637144       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:08.640833       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:08.645088       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:10.650083       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:10.654415       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:12.657425       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:12.661532       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:14.664511       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:14.671415       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:16.675561       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:16.680148       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:18.683347       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:18.687021       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:20.690815       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:20.694936       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:22.698653       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:22.702694       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:24.706485       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:24.711047       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:26.715785       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:26.723407       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:28.725981       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:28.730114       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:30.732881       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:30.736976       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:32.740920       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:32.745151       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:34.748833       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:34.755179       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:36.759450       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:36.762496       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:38.766359       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:38.774299       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:40.777929       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:40.782846       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:42.787314       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:42.791306       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:44.794629       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:44.799606       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:46.803027       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:46.810826       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:48.814712       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:48.819050       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:50.822308       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:50.827676       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:52.832185       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1015 05:37:52.837353       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

